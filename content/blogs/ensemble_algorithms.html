---
title: "Predicting London House Prices"
date: "2022-09-18"
description: Predicting London House Prices using Ensemble algorithms with R
draft: no
image: ensemble_algos.jpg
keywords: ''
slug: ensemble_algorithms
categories:
- ''
- ''
---



<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>
<div id="introduction-and-learning-objectives" class="section level1">
<h1>Introduction and learning objectives</h1>
<div class="navy1">
<p>The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.</p>
<p><b>Learning objectives</b></p>
<ol type="i">
<li>
Using different data mining algorithms for prediction.
</li>
<li>
Dealing with large data sets
</li>
<li>
Tuning data mining algorithms
</li>
<li>
Interpreting data mining algorithms and deducing importance of variables
</li>
<li>
Using results of data mining algorithms to make business decisions
</li>
</ol>
</div>
</div>
<div id="load-data" class="section level1">
<h1>Load data</h1>
<p>There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets.</p>
<p>Make sure you understand what information each column contains. Note that not all information provided might be useful in predicting house prices, but do not make any assumptions before you decide what information you use in your prediction algorithms.</p>
<pre class="r"><code>#read in the data

london_house_prices_2019_training&lt;-read.csv(&quot;C:/Users/optif/Desktop/academics/applied_statistics/Francesco_repository_1/data/training_data_assignment_with_prices.csv&quot;)
london_house_prices_2019_out_of_sample&lt;-read.csv(&quot;C:/Users/optif/Desktop/academics/applied_statistics/Francesco_repository_1/data/test_data_assignment.csv&quot;)

#fix data types in both data sets

#fix dates
london_house_prices_2019_training &lt;- london_house_prices_2019_training %&gt;% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample&lt;-london_house_prices_2019_out_of_sample %&gt;% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training &lt;- london_house_prices_2019_training %&gt;% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample&lt;-london_house_prices_2019_out_of_sample %&gt;% mutate_if(is.character,as.factor)

#take a quick look at what&#39;s in the data
str(london_house_prices_2019_training)</code></pre>
<pre><code>## &#39;data.frame&#39;:	13998 obs. of  37 variables:
##  $ ID                          : int  2 3 4 5 7 8 9 10 11 12 ...
##  $ date                        : Date, format: &quot;2019-11-01&quot; &quot;2019-08-08&quot; ...
##  $ postcode                    : Factor w/ 12635 levels &quot;BR1 1AB&quot;,&quot;BR1 1LR&quot;,..: 10897 11027 11264 2031 11241 11066 421 9594 9444 873 ...
##  $ property_type               : Factor w/ 4 levels &quot;D&quot;,&quot;F&quot;,&quot;S&quot;,&quot;T&quot;: 2 2 3 2 3 2 1 4 4 2 ...
##  $ whether_old_or_new          : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ freehold_or_leasehold       : Factor w/ 2 levels &quot;F&quot;,&quot;L&quot;: 2 2 1 2 1 2 1 1 1 2 ...
##  $ address1                    : Factor w/ 2825 levels &quot;1&quot;,&quot;1 - 2&quot;,&quot;1 - 3&quot;,..: 2503 792 253 789 569 234 264 418 5 274 ...
##  $ address2                    : Factor w/ 434 levels &quot;1&quot;,&quot;10&quot;,&quot;101&quot;,..: 372 NA NA NA NA NA NA NA NA NA ...
##  $ address3                    : Factor w/ 8543 levels &quot;ABBERTON WALK&quot;,..: 6990 6821 3715 2492 4168 2879 3620 5251 6045 6892 ...
##  $ town                        : Factor w/ 133 levels &quot;ABBEY WOOD&quot;,&quot;ACTON&quot;,..: NA NA NA 78 NA NA NA NA NA NA ...
##  $ local_aut                   : Factor w/ 69 levels &quot;ASHFORD&quot;,&quot;BARKING&quot;,..: 36 46 24 36 24 46 65 36 36 17 ...
##  $ county                      : Factor w/ 33 levels &quot;BARKING AND DAGENHAM&quot;,..: 22 27 18 25 18 27 5 27 32 8 ...
##  $ postcode_short              : Factor w/ 247 levels &quot;BR1&quot;,&quot;BR2&quot;,&quot;BR3&quot;,..: 190 194 198 28 198 194 4 169 167 8 ...
##  $ current_energy_rating       : Factor w/ 6 levels &quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,..: 4 3 3 4 3 2 4 3 4 2 ...
##  $ total_floor_area            : num  30 50 100 39 88 101 136 148 186 65 ...
##  $ number_habitable_rooms      : int  2 2 5 2 4 4 6 6 6 3 ...
##  $ co2_emissions_current       : num  2.3 3 3.7 2.8 3.9 3.1 8.1 5.6 10 1.5 ...
##  $ co2_emissions_potential     : num  1.7 1.7 1.5 1.1 1.4 1.4 4.1 2 6.1 1.5 ...
##  $ energy_consumption_current  : int  463 313 212 374 251 175 339 216 308 128 ...
##  $ energy_consumption_potential: int  344 175 82 144 90 77 168 75 186 128 ...
##  $ windows_energy_eff          : Factor w/ 5 levels &quot;Average&quot;,&quot;Good&quot;,..: 1 1 1 5 1 1 1 1 5 1 ...
##  $ tenure                      : Factor w/ 3 levels &quot;owner-occupied&quot;,..: 1 2 1 2 1 1 1 2 1 1 ...
##  $ latitude                    : num  51.5 51.5 51.5 51.6 51.5 ...
##  $ longitude                   : num  -0.1229 -0.2828 -0.4315 0.0423 -0.4293 ...
##  $ population                  : int  34 75 83 211 73 51 25 91 60 97 ...
##  $ altitude                    : int  8 9 25 11 21 11 95 7 7 106 ...
##  $ london_zone                 : int  1 3 5 3 6 6 3 2 2 3 ...
##  $ nearest_station             : Factor w/ 592 levels &quot;abbey road&quot;,&quot;abbey wood&quot;,..: 478 358 235 319 180 502 566 30 32 566 ...
##  $ water_company               : Factor w/ 5 levels &quot;Affinity Water&quot;,..: 5 5 1 5 1 5 5 5 5 5 ...
##  $ average_income              : int  57200 61900 50600 45400 49000 56200 57200 65600 50400 52300 ...
##  $ district                    : Factor w/ 33 levels &quot;Barking and Dagenham&quot;,..: 22 27 18 26 18 27 5 27 32 8 ...
##  $ price                       : num  360000 408500 499950 259999 395000 ...
##  $ type_of_closest_station     : Factor w/ 3 levels &quot;light_rail&quot;,&quot;rail&quot;,..: 3 2 3 1 3 2 1 3 1 1 ...
##  $ num_tube_lines              : int  1 0 1 0 1 0 0 2 0 0 ...
##  $ num_rail_lines              : int  0 1 1 0 1 1 0 0 1 0 ...
##  $ num_light_rail_lines        : int  0 0 0 1 0 0 1 0 1 1 ...
##  $ distance_to_station         : num  0.528 0.77 0.853 0.29 1.073 ...</code></pre>
<pre class="r"><code>str(london_house_prices_2019_out_of_sample)</code></pre>
<pre><code>## &#39;data.frame&#39;:	1999 obs. of  37 variables:
##  $ ID                          : int  14434 12562 8866 10721 1057 1527 13961 12108 9363 1155 ...
##  $ date                        : Date, format: NA NA ...
##  $ postcode                    : logi  NA NA NA NA NA NA ...
##  $ property_type               : Factor w/ 4 levels &quot;D&quot;,&quot;F&quot;,&quot;S&quot;,&quot;T&quot;: 1 2 2 3 4 3 2 3 2 4 ...
##  $ whether_old_or_new          : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ freehold_or_leasehold       : Factor w/ 2 levels &quot;F&quot;,&quot;L&quot;: 1 2 2 1 1 1 2 1 2 1 ...
##  $ address1                    : logi  NA NA NA NA NA NA ...
##  $ address2                    : logi  NA NA NA NA NA NA ...
##  $ address3                    : logi  NA NA NA NA NA NA ...
##  $ town                        : Factor w/ 54 levels &quot;ACTON&quot;,&quot;ADDISCOMBE&quot;,..: NA NA NA NA NA NA NA NA NA NA ...
##  $ local_aut                   : logi  NA NA NA NA NA NA ...
##  $ county                      : logi  NA NA NA NA NA NA ...
##  $ postcode_short              : Factor w/ 221 levels &quot;BR1&quot;,&quot;BR2&quot;,&quot;BR3&quot;,..: 82 50 37 52 214 150 159 115 175 126 ...
##  $ current_energy_rating       : Factor w/ 6 levels &quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,..: 3 2 3 3 4 4 4 3 4 3 ...
##  $ total_floor_area            : num  150 59 58 74 97.3 ...
##  $ number_habitable_rooms      : int  6 2 2 5 5 5 5 4 2 5 ...
##  $ co2_emissions_current       : num  7.3 1.5 2.8 3.5 6.5 4.9 5.1 2.9 4.2 4.3 ...
##  $ co2_emissions_potential     : num  2.4 1.4 1.2 1.2 5.7 1.6 3 0.8 3.2 2.5 ...
##  $ energy_consumption_current  : int  274 142 253 256 303 309 240 224 458 253 ...
##  $ energy_consumption_potential: int  89 136 110 80 266 101 140 58 357 143 ...
##  $ windows_energy_eff          : Factor w/ 5 levels &quot;Average&quot;,&quot;Good&quot;,..: 1 1 1 1 1 1 3 1 3 1 ...
##  $ tenure                      : Factor w/ 3 levels &quot;owner-occupied&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ latitude                    : num  51.6 51.6 51.5 51.6 51.5 ...
##  $ longitude                   : num  -0.129 -0.2966 -0.0328 -0.3744 -0.2576 ...
##  $ population                  : int  87 79 23 73 100 24 22 49 65 98 ...
##  $ altitude                    : int  63 38 17 39 8 46 26 16 14 18 ...
##  $ london_zone                 : int  4 4 2 5 2 4 3 6 1 3 ...
##  $ nearest_station             : Factor w/ 494 levels &quot;abbey wood&quot;,&quot;acton central&quot;,..: 16 454 181 302 431 142 20 434 122 212 ...
##  $ water_company               : Factor w/ 4 levels &quot;Affinity Water&quot;,..: 4 1 4 1 4 4 4 2 4 4 ...
##  $ average_income              : int  61300 48900 46200 52200 60700 59600 64000 48100 56600 53500 ...
##  $ district                    : Factor w/ 32 levels &quot;Barking and Dagenham&quot;,..: 9 4 29 14 17 10 31 15 19 22 ...
##  $ type_of_closest_station     : Factor w/ 3 levels &quot;light_rail&quot;,&quot;rail&quot;,..: 3 3 1 2 3 2 3 3 3 2 ...
##  $ num_tube_lines              : int  1 2 0 0 2 0 1 1 2 0 ...
##  $ num_rail_lines              : int  0 1 0 1 0 1 1 0 0 1 ...
##  $ num_light_rail_lines        : int  0 1 1 0 0 0 0 1 0 0 ...
##  $ distance_to_station         : num  0.839 0.104 0.914 0.766 0.449 ...
##  $ asking_price                : num  750000 229000 152000 379000 930000 350000 688000 386000 534000 459000 ...</code></pre>
<pre class="r"><code>#dropping columns with missing values
london_house_prices_2019_training &lt;- london_house_prices_2019_training %&gt;%
  select(-c(address2, town, population))

#let&#39;s do the initial split
library(rsample)
set.seed(123)
train_test_split &lt;- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset
train_data &lt;- training(train_test_split)
test_data &lt;- testing(train_test_split)</code></pre>
</div>
<div id="visualize-data" class="section level1">
<h1>Visualize data</h1>
<p>Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?</p>
<pre class="r"><code># Analyzing the dataset
skimr::skim(london_house_prices_2019_training)</code></pre>
<table>
<caption><span id="tab:visualize">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">london_house_prices_2019_…</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">13998</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">34</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Date</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">factor</td>
<td align="left">16</td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">17</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: Date</strong></p>
<table>
<colgroup>
<col width="17%" />
<col width="12%" />
<col width="17%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">min</th>
<th align="left">max</th>
<th align="left">median</th>
<th align="right">n_unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">date</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">2019-01-02</td>
<td align="left">2019-12-30</td>
<td align="left">2019-07-22</td>
<td align="right">263</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<colgroup>
<col width="22%" />
<col width="9%" />
<col width="12%" />
<col width="7%" />
<col width="8%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">postcode</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">12635</td>
<td align="left">E11: 5, BR1: 4, CR0: 4, E17: 4</td>
</tr>
<tr class="even">
<td align="left">property_type</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">4</td>
<td align="left">F: 5402, T: 4999, S: 2780, D: 817</td>
</tr>
<tr class="odd">
<td align="left">whether_old_or_new</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">N: 13990, Y: 8</td>
</tr>
<tr class="even">
<td align="left">freehold_or_leasehold</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">F: 8457, L: 5541</td>
</tr>
<tr class="odd">
<td align="left">address1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2825</td>
<td align="left">3: 220, 7: 212, 4: 204, 12: 197</td>
</tr>
<tr class="even">
<td align="left">address3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">8543</td>
<td align="left">LON: 25, GRE: 24, THE: 23, HIG: 21</td>
</tr>
<tr class="odd">
<td align="left">local_aut</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">69</td>
<td align="left">LON: 7512, ROM: 396, BRO: 277, CRO: 243</td>
</tr>
<tr class="even">
<td align="left">county</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">33</td>
<td align="left">BRO: 861, CRO: 729, WAN: 702, HAV: 672</td>
</tr>
<tr class="odd">
<td align="left">postcode_short</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">247</td>
<td align="left">CR0: 243, SW1: 204, E17: 193, SW1: 187</td>
</tr>
<tr class="even">
<td align="left">current_energy_rating</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">6</td>
<td align="left">D: 7075, C: 3488, E: 2646, B: 361</td>
</tr>
<tr class="odd">
<td align="left">windows_energy_eff</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">5</td>
<td align="left">Ave: 7819, Goo: 3244, Ver: 1698, Poo: 1231</td>
</tr>
<tr class="even">
<td align="left">tenure</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">own: 11236, ren: 2503, ren: 259</td>
</tr>
<tr class="odd">
<td align="left">nearest_station</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">592</td>
<td align="left">rom: 201, cha: 103, bex: 99, har: 96</td>
</tr>
<tr class="even">
<td align="left">water_company</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">5</td>
<td align="left">Tha: 10471, Aff: 1577, Ess: 1154, SES: 792</td>
</tr>
<tr class="odd">
<td align="left">district</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">33</td>
<td align="left">Cro: 925, Bro: 843, Hav: 681, Bex: 603</td>
</tr>
<tr class="even">
<td align="left">type_of_closest_station</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">rai: 6512, tub: 4711, lig: 2775</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="23%" />
<col width="8%" />
<col width="11%" />
<col width="8%" />
<col width="7%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="7%" />
<col width="4%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">8003.94</td>
<td align="right">4.62e+03</td>
<td align="right">2.00</td>
<td align="right">4004.50</td>
<td align="right">8015.50</td>
<td align="right">1.20e+04</td>
<td align="right">1.60e+04</td>
<td align="left">▇▇▇▇▇</td>
</tr>
<tr class="even">
<td align="left">total_floor_area</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">92.55</td>
<td align="right">4.51e+01</td>
<td align="right">21.00</td>
<td align="right">64.00</td>
<td align="right">83.00</td>
<td align="right">1.08e+02</td>
<td align="right">4.80e+02</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">number_habitable_rooms</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.31</td>
<td align="right">1.66e+00</td>
<td align="right">1.00</td>
<td align="right">3.00</td>
<td align="right">4.00</td>
<td align="right">5.00e+00</td>
<td align="right">1.40e+01</td>
<td align="left">▆▇▁▁▁</td>
</tr>
<tr class="even">
<td align="left">co2_emissions_current</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4.25</td>
<td align="right">2.39e+00</td>
<td align="right">0.10</td>
<td align="right">2.70</td>
<td align="right">3.80</td>
<td align="right">5.20e+00</td>
<td align="right">4.40e+01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">co2_emissions_potential</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2.22</td>
<td align="right">1.44e+00</td>
<td align="right">-0.50</td>
<td align="right">1.30</td>
<td align="right">1.80</td>
<td align="right">2.70e+00</td>
<td align="right">2.05e+01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">energy_consumption_current</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">262.98</td>
<td align="right">9.34e+01</td>
<td align="right">12.00</td>
<td align="right">202.00</td>
<td align="right">249.00</td>
<td align="right">3.08e+02</td>
<td align="right">1.30e+03</td>
<td align="left">▇▅▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">energy_consumption_potential</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">141.38</td>
<td align="right">7.81e+01</td>
<td align="right">-49.00</td>
<td align="right">89.00</td>
<td align="right">122.00</td>
<td align="right">1.67e+02</td>
<td align="right">1.02e+03</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="even">
<td align="left">latitude</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">51.49</td>
<td align="right">8.00e-02</td>
<td align="right">51.30</td>
<td align="right">51.43</td>
<td align="right">51.49</td>
<td align="right">5.16e+01</td>
<td align="right">5.17e+01</td>
<td align="left">▂▇▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">longitude</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-0.11</td>
<td align="right">1.60e-01</td>
<td align="right">-0.49</td>
<td align="right">-0.21</td>
<td align="right">-0.11</td>
<td align="right">0.00e+00</td>
<td align="right">2.90e-01</td>
<td align="left">▂▅▇▅▂</td>
</tr>
<tr class="even">
<td align="left">altitude</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">36.57</td>
<td align="right">2.60e+01</td>
<td align="right">0.00</td>
<td align="right">16.00</td>
<td align="right">32.00</td>
<td align="right">5.10e+01</td>
<td align="right">2.39e+02</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">london_zone</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.76</td>
<td align="right">1.43e+00</td>
<td align="right">1.00</td>
<td align="right">3.00</td>
<td align="right">4.00</td>
<td align="right">5.00e+00</td>
<td align="right">7.00e+00</td>
<td align="left">▇▇▇▆▅</td>
</tr>
<tr class="even">
<td align="left">average_income</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">55334.70</td>
<td align="right">8.45e+03</td>
<td align="right">36000.00</td>
<td align="right">49400.00</td>
<td align="right">54600.00</td>
<td align="right">6.06e+04</td>
<td align="right">8.52e+04</td>
<td align="left">▃▇▆▂▁</td>
</tr>
<tr class="odd">
<td align="left">price</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">593790.93</td>
<td align="right">5.19e+05</td>
<td align="right">77000.00</td>
<td align="right">351000.00</td>
<td align="right">460000.00</td>
<td align="right">6.50e+05</td>
<td align="right">1.08e+07</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">num_tube_lines</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.44</td>
<td align="right">7.40e-01</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00e+00</td>
<td align="right">6.00e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">num_rail_lines</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.58</td>
<td align="right">5.10e-01</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">1.00e+00</td>
<td align="right">2.00e+00</td>
<td align="left">▆▁▇▁▁</td>
</tr>
<tr class="even">
<td align="left">num_light_rail_lines</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.24</td>
<td align="right">4.30e-01</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00e+00</td>
<td align="right">1.00e+00</td>
<td align="left">▇▁▁▁▂</td>
</tr>
<tr class="odd">
<td align="left">distance_to_station</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.65</td>
<td align="right">4.00e-01</td>
<td align="right">0.00</td>
<td align="right">0.37</td>
<td align="right">0.57</td>
<td align="right">8.40e-01</td>
<td align="right">5.61e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code># Let&#39;s first see how the response variable&#39;s distribution looks like in the
#training data
ggplot(london_house_prices_2019_training, aes(x=log(price))) +
  geom_density()</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Visualizing distribution of numerical variables highly correlated with price
# distance_to_station, highly skewed variable (-0.16)
ggplot(london_house_prices_2019_training, aes(x=distance_to_station)) +
  geom_density()</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(london_house_prices_2019_training, aes(x=distance_to_station, y=log(price))) +
  geom_point() + geom_smooth(method = &#39;lm&#39;, se = TRUE)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-3.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># average_income (0.42)
ggplot(london_house_prices_2019_training, aes(x=average_income)) +
  geom_density()</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-4.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(london_house_prices_2019_training, aes(x=average_income, y=log(price))) +
  geom_point() + geom_smooth(method = &#39;lm&#39;, se = TRUE)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-5.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># co2_emissions_current (0.58)
ggplot(london_house_prices_2019_training, aes(x=co2_emissions_current)) +
  geom_density()</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-6.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(london_house_prices_2019_training, aes(x=co2_emissions_potential, y=log(price))) +
  geom_point() + geom_smooth(method = &#39;lm&#39;, se = TRUE)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-7.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># total_floor_area (0.74, highly correlated with co2_emissions_potential)
ggplot(london_house_prices_2019_training, aes(x=total_floor_area)) +
  geom_density()</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-8.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(london_house_prices_2019_training, aes(x=total_floor_area, y=log(price))) +
  geom_point() + geom_smooth(method = &#39;lm&#39;, se = TRUE)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-9.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Visualizing relationship with qualitative variables
# london_zone (moderate correlation)
london_house_prices_2019_training %&gt;%
  group_by(london_zone) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=london_zone,
             y=median_price,
             fill=london_zone)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-10.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># property_type (moderate correlation)
london_house_prices_2019_training %&gt;%
  group_by(property_type) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=property_type,
             y=median_price,
             fill=property_type)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-11.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># freehold_or_leasehold (moderate correlation)
london_house_prices_2019_training %&gt;%
  group_by(freehold_or_leasehold) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=freehold_or_leasehold,
             y=median_price,
             fill=freehold_or_leasehold)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-12.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># current_energy_rating (moderate correlation - high cor with co2_emissions)
london_house_prices_2019_training %&gt;%
  group_by(current_energy_rating) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=current_energy_rating,
             y=median_price,
             fill=current_energy_rating)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-13.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># tenure (weak correlation)
london_house_prices_2019_training %&gt;%
  group_by(tenure) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=tenure,
             y=median_price,
             fill=tenure)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-14.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># water_company (moderate to high correlation)
london_house_prices_2019_training %&gt;%
  group_by(water_company) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ggplot(aes(x=water_company,
             y=median_price,
             fill=water_company)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-15.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># type_of_closest_station (moderate to high correlation)
london_house_prices_2019_training %&gt;%
  group_by(type_of_closest_station) %&gt;%
  summarise(median_price = mean(price)) %&gt;%
  ggplot(aes(x=type_of_closest_station,
             y=median_price,
             fill=type_of_closest_station)) +
  geom_bar(stat = &#39;identity&#39;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/visualize-16.png" width="648" style="display: block; margin: auto;" />
&gt; First, it is important to skimr::skim() the data set in order to get a rough
sense of the main features of the data: missing data, minimum, maximum, rough
representation of the distribution. In addition, it is crucial to visualise the
distribution of continuous variables (especially those most correlated with the
output variable we are trying to predict), in order to get a sense of their
skewness and correlation with the price. In fact, we do the same with categorical
variables: we visualize how those variables affect the price.</p>
<blockquote>
<p>Through these visualisations, we can learn the type and magnitude of the
correlation with the output variable for each input variable and choose which to
include in our models accordingly.</p>
</blockquote>
<p>Estimate a correlation table between prices and other continuous variables. What do you glean from the correlation table?</p>
<pre class="r"><code># produce a correlation table using GGally::ggcor()
# this takes a while to plot

library(&quot;GGally&quot;)
london_house_prices_2019_training %&gt;% 
  select(-ID) %&gt;% #keep Y variable last
  mutate(price = log(price)) %&gt;%
  ggcorr(method = c(&quot;pairwise&quot;, &quot;pearson&quot;), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2,angle = -20)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/correlation%20table-1.png" width="648" style="display: block; margin: auto;" />
&gt; The correlation table is a crucial tool when it comes to build predictive
models: it allows to get a sense of the magnitude of the correlation among all
the numerical variables in the dataset. In this case, we can see how price is highly
correlated with average_income, num_tube_lines, london_zone (better if read as a
factor), co2_emissions_potential, co2_emissions_current, number_habitable_rooms
and total_floor_area.</p>
<blockquote>
<p>It is also possible to get a sense of the correlation among regressors: the
correlation table is extremely useful when it comes to assessing the potential
of multicolinearity in the model, due to correlation among predictors.</p>
</blockquote>
</div>
<div id="fit-a-linear-regression-model" class="section level1">
<h1>Fit a linear regression model</h1>
<p>To help you get started I build a linear regression model below. I chose a subset of the features with no particular goal. You can (and should) add more variables and/or choose variable selection methods if you want.</p>
<pre class="r"><code>#Define control variables
control &lt;- trainControl(
    method=&quot;cv&quot;,
    number=5,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

set.seed(987)
#We are going to train the model on the training data and select the features
#using the stepwise selection method
lm_model &lt;- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = &quot;leapBackward&quot;,
  tuneGrid = data.frame(nvmax = 67:77),
  trControl = control
)</code></pre>
<pre><code>## + Fold1: nvmax=77 
## - Fold1: nvmax=77 
## + Fold2: nvmax=77 
## - Fold2: nvmax=77 
## + Fold3: nvmax=77 
## - Fold3: nvmax=77 
## + Fold4: nvmax=77 
## - Fold4: nvmax=77 
## + Fold5: nvmax=77 
## - Fold5: nvmax=77 
## Aggregating results
## Selecting tuning parameters
## Fitting nvmax = 69 on full training set</code></pre>
<pre class="r"><code>#linear check
linear &lt;- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = &quot;lm&quot;,
  trControl = control
)</code></pre>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code>#shows results of all models
lm_model$results</code></pre>
<pre><code>##    nvmax  RMSE Rsquared  MAE  RMSESD RsquaredSD   MAESD
## 1     67 0.235    0.818 0.17 0.00596     0.0110 0.00431
## 2     68 0.235    0.818 0.17 0.00595     0.0110 0.00430
## 3     69 0.235    0.818 0.17 0.00597     0.0110 0.00432
## 4     70 0.235    0.818 0.17 0.00599     0.0111 0.00434
## 5     71 0.235    0.818 0.17 0.00601     0.0111 0.00434
## 6     72 0.235    0.818 0.17 0.00600     0.0111 0.00433
## 7     73 0.235    0.818 0.17 0.00601     0.0111 0.00433
## 8     74 0.236    0.817 0.17 0.00589     0.0105 0.00425
## 9     75 0.236    0.817 0.17 0.00590     0.0106 0.00425
## 10    76 0.236    0.817 0.17 0.00590     0.0106 0.00425
## 11    77 0.236    0.817 0.17 0.00590     0.0106 0.00425</code></pre>
<pre class="r"><code>#summarizes model of best fit&#39;s coefficients
coef(lm_model$finalModel, lm_model$bestTune$nvmax)</code></pre>
<pre><code>##                                     (Intercept) 
##                                        1.20e+01 
##                                total_floor_area 
##                                        4.97e-03 
##                                  districtBarnet 
##                                        2.29e-01 
##                                  districtBexley 
##                                       -9.49e-02 
##                                   districtBrent 
##                                        7.69e-02 
##                                 districtBromley 
##                                       -2.08e-02 
##                                  districtCamden 
##                                        3.49e-01 
##                          districtCity of London 
##                                       -2.44e-01 
##                                 districtCroydon 
##                                        1.49e-02 
##                                  districtEaling 
##                                        9.20e-02 
##                                 districtEnfield 
##                                        1.68e-01 
##                               districtGreenwich 
##                                       -8.87e-02 
##                                 districtHackney 
##                                        1.60e-01 
##                  districtHammersmith and Fulham 
##                                        2.15e-01 
##                                districtHaringey 
##                                        1.82e-01 
##                                  districtHarrow 
##                                        1.92e-01 
##                                districtHavering 
##                                        1.49e-01 
##                              districtHillingdon 
##                                        1.73e-01 
##                               districtIslington 
##                                        6.53e-02 
##                  districtKensington and Chelsea 
##                                        6.00e-01 
##                    districtKingston upon Thames 
##                                        1.24e-01 
##                                 districtLambeth 
##                                        2.12e-01 
##                                  districtMerton 
##                                       -6.72e-02 
##                                  districtNewham 
##                                       -3.88e-02 
##                               districtRedbridge 
##                                        4.06e-02 
##                    districtRichmond upon Thames 
##                                        1.98e-01 
##                               districtSouthwark 
##                                        1.22e-01 
##                                  districtSutton 
##                                       -6.02e-02 
##                           districtTower Hamlets 
##                                       -1.44e-01 
##                          districtWaltham Forest 
##                                        1.52e-01 
##                              districtWandsworth 
##                                        1.30e-01 
##                             districtWestminster 
##                                        3.01e-01 
##              water_companyEssex &amp; Suffolk Water 
##                                        3.01e-02 
##                     water_companyLeep Utilities 
##                                        1.72e-01 
##                          water_companySES Water 
##                                        1.36e-01 
##                       water_companyThames Water 
##                                        1.39e-01 
##                                  property_typeF 
##                                       -3.95e-01 
##                                  property_typeS 
##                                       -9.64e-02 
##                                  property_typeT 
##                                       -1.49e-01 
##                                     london_zone 
##                                       -8.42e-02 
##                                  num_tube_lines 
##                                        1.98e-02 
##                           co2_emissions_current 
##                                        3.89e-03 
##                          number_habitable_rooms 
##                                        3.04e-02 
##                                  average_income 
##                                        1.37e-05 
##                  total_floor_area:districtBrent 
##                                        1.51e-03 
##                 total_floor_area:districtCamden 
##                                        9.41e-04 
##         total_floor_area:districtCity of London 
##                                        1.09e-02 
##                total_floor_area:districtCroydon 
##                                       -1.19e-03 
##                 total_floor_area:districtEaling 
##                                        3.96e-04 
##                total_floor_area:districtEnfield 
##                                       -8.30e-04 
##              total_floor_area:districtGreenwich 
##                                        7.44e-04 
##                total_floor_area:districtHackney 
##                                        1.50e-03 
## total_floor_area:districtHammersmith and Fulham 
##                                        1.89e-03 
##               total_floor_area:districtHaringey 
##                                        5.66e-04 
##               total_floor_area:districtHavering 
##                                       -2.20e-04 
##               total_floor_area:districtHounslow 
##                                        1.37e-03 
##              total_floor_area:districtIslington 
##                                        3.47e-03 
## total_floor_area:districtKensington and Chelsea 
##                                        1.54e-03 
##                total_floor_area:districtLambeth 
##                                       -3.91e-04 
##               total_floor_area:districtLewisham 
##                                       -1.77e-04 
##                 total_floor_area:districtMerton 
##                                        9.34e-04 
##                 total_floor_area:districtNewham 
##                                       -7.03e-04 
##              total_floor_area:districtRedbridge 
##                                       -4.24e-04 
##   total_floor_area:districtRichmond upon Thames 
##                                        7.37e-04 
##              total_floor_area:districtSouthwark 
##                                        8.63e-04 
##                 total_floor_area:districtSutton 
##                                       -4.31e-04 
##          total_floor_area:districtTower Hamlets 
##                                        3.16e-03 
##         total_floor_area:districtWaltham Forest 
##                                       -5.51e-04 
##             total_floor_area:districtWandsworth 
##                                        6.39e-04 
##            total_floor_area:districtWestminster 
##                                        3.59e-03</code></pre>
<pre class="r"><code># we can check variable importance as well
importance &lt;- varImp(lm_model, scale=TRUE)
plot(importance)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/variable%20importance%20check-1.png" width="648" style="display: block; margin: auto;" /></p>
<div id="predict-the-values-in-testing-and-out-of-sample-data" class="section level2">
<h2>Predict the values in testing and out of sample data</h2>
<p>Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?</p>
<pre class="r"><code>#we can predict the testing values
predictions &lt;- predict(lm_model,test_data)

lr_results&lt;-data.frame(RMSE = RMSE(predictions, log(test_data$price)),
                       Rsquare = R2(predictions, log(test_data$price)))

lr_results</code></pre>
<pre><code>##    RMSE Rsquare
## 1 0.236   0.804</code></pre>
<blockquote>
<p>The quality of a linear regression model can be measured by the RMSE and the
Rsquare. The RMSE (Root Mean Squared Error) measures how much do the predictions
of the model differ from the actual output values. The R-squared measures what
percentage of the variance in the output variable is explained by the model. In
orderly to properly test the quality of our predictions, it’s crucial to compute
the RMSE and R2 indices on the testing data before computing them on the testing
data in order to evaluate if the model tends to overfit the training data.</p>
</blockquote>
</div>
</div>
<div id="fit-a-tree-model" class="section level1">
<h1>Fit a tree model</h1>
<p>Next I fit a tree model using the same subset of features. Again you can (and should) add more variables and tune the parameter of your tree to find a better fit.</p>
<p>Compare the performance of the linear regression model with the tree model; which one performs better? Why do you think that is the case?</p>
<pre class="r"><code>#Tuning the model
tree_model &lt;- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = &quot;rpart&quot;,
  trControl = control,
  tuneLength=20
    )</code></pre>
<pre><code>## + Fold1: cp=0.004063 
## - Fold1: cp=0.004063 
## + Fold2: cp=0.004063 
## - Fold2: cp=0.004063 
## + Fold3: cp=0.004063 
## - Fold3: cp=0.004063 
## + Fold4: cp=0.004063 
## - Fold4: cp=0.004063 
## + Fold5: cp=0.004063 
## - Fold5: cp=0.004063 
## Aggregating results
## Selecting tuning parameters
## Fitting cp = 0.00406 on full training set</code></pre>
<pre class="r"><code>#You can view how the tree performs
tree_model$results</code></pre>
<pre><code>##         cp  RMSE Rsquared   MAE  RMSESD RsquaredSD   MAESD
## 1  0.00406 0.299    0.707 0.223 0.00531    0.01167 0.00281
## 2  0.00452 0.300    0.704 0.225 0.00543    0.01159 0.00266
## 3  0.00478 0.302    0.700 0.226 0.00505    0.01159 0.00293
## 4  0.00494 0.306    0.692 0.228 0.00420    0.01256 0.00339
## 5  0.00497 0.306    0.691 0.229 0.00387    0.01429 0.00293
## 6  0.00508 0.306    0.691 0.229 0.00387    0.01429 0.00293
## 7  0.00521 0.308    0.688 0.230 0.00361    0.01481 0.00187
## 8  0.00730 0.317    0.669 0.237 0.00579    0.01852 0.00271
## 9  0.00758 0.318    0.668 0.238 0.00504    0.01777 0.00289
## 10 0.00803 0.319    0.664 0.239 0.00444    0.01935 0.00219
## 11 0.01203 0.326    0.650 0.244 0.00691    0.01546 0.00435
## 12 0.01322 0.331    0.640 0.248 0.00533    0.02076 0.00384
## 13 0.01626 0.336    0.628 0.253 0.00313    0.01194 0.00222
## 14 0.01773 0.345    0.609 0.259 0.00578    0.01821 0.00444
## 15 0.02676 0.353    0.590 0.266 0.00577    0.02423 0.00396
## 16 0.03568 0.377    0.533 0.285 0.00999    0.02417 0.00704
## 17 0.03713 0.388    0.506 0.294 0.01111    0.01600 0.01143
## 18 0.06682 0.397    0.480 0.302 0.01064    0.02932 0.00581
## 19 0.08025 0.432    0.386 0.325 0.01920    0.04570 0.01119
## 20 0.35693 0.487    0.353 0.365 0.05952    0.00182 0.04494</code></pre>
<pre class="r"><code>#You can view the final tree
rpart.plot(tree_model$finalModel)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/tree%20model-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># We can predict the testing values
predictions_tree &lt;- predict(tree_model, test_data)

tree_results&lt;-data.frame(RMSE = RMSE(predictions_tree, log(test_data$price)),
                         Rsquare = R2(predictions_tree, log(test_data$price)))

tree_results</code></pre>
<pre><code>##    RMSE Rsquare
## 1 0.299   0.686</code></pre>
<blockquote>
<p>Assuming that the response variable is the logarithm of the price (instead of
the price itself), we can see that the linear regression performs significantly
better than the tree algorithm. In fact, the RMSE and Rsquare for linear
regression are 0.2359316 and 0.8038051 respectively, whereas the same indices
for the tree model are 0.2985389 and 0.6863361 respectively.</p>
</blockquote>
<blockquote>
<p>Tree algorithms usually perform better than linear regressions when the
relationship between the predictor variables and the response variable is
non-linear. However, when the relationship between response and predictor
variables is at least approximately linear, the linear regression performs
better. This happens because a linear increase in the predicted value - caused
by a linear increase in the predictors - allows for a lower error than the one
obtained when the predicted value stays the same even if predictors change,
albeit the change is within a certain margin (that is the margin defined by the
different splits in the model).</p>
</blockquote>
<blockquote>
<p>In addition, in order to increase the performance of trees, we could increase
the number of levels. However, this would make the model difficult to interpret.</p>
</blockquote>
</div>
<div id="k-nn-model" class="section level1">
<h1>K-NN model</h1>
<pre class="r"><code>set.seed(456)

#fitting the model
knn_model &lt;- train(log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
                   train_data,
                   method = &quot;knn&quot;,
                   trControl = trainControl(&quot;cv&quot;, number = 10), #use 10 fold cross validation
                   tuneLength = 10, #number of parameter values tried by function
                   preProcess = c(&quot;center&quot;, &quot;scale&quot;))

#showing model results
knn_model</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 10498 samples
##     9 predictor
## 
## Pre-processing: centered (77), scaled (77) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 9448, 9448, 9447, 9448, 9447, 9448, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE   Rsquared  MAE  
##    5  0.239  0.813     0.171
##    7  0.237  0.816     0.170
##    9  0.237  0.817     0.170
##   11  0.238  0.816     0.171
##   13  0.239  0.815     0.171
##   15  0.240  0.814     0.172
##   17  0.241  0.813     0.173
##   19  0.243  0.812     0.174
##   21  0.245  0.810     0.175
##   23  0.246  0.809     0.177
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 7.</code></pre>
<pre class="r"><code>plot(knn_model)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/K-NN%20initial%20fit-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#visualising the importance of each variable in knn_model
importance_knn &lt;- varImp(knn_model, scale=TRUE)
plot(importance_knn)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/K-NN%20initial%20fit-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#fit knn with expand.grid including more neighbors
knnGrid &lt;-  expand.grid(k= seq(10,100 , by = 10)) 

set.seed(789)

#the metric used to choose k is the default &quot;RMSE&quot;
knn_model2 &lt;- train(log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
                    train_data,
                    preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                    method=&quot;knn&quot;,
                    trControl=trainControl(&quot;cv&quot;, number = 10),
                    tuneGrid = knnGrid)

#display importance of variables
importance_knn2 &lt;- varImp(knn_model2, scale=TRUE)
plot(importance_knn2)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/K-NN%20more%20neighbors-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># display results
print(knn_model2)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 10498 samples
##     9 predictor
## 
## Pre-processing: centered (77), scaled (77) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 9448, 9450, 9448, 9448, 9448, 9448, ... 
## Resampling results across tuning parameters:
## 
##   k    RMSE   Rsquared  MAE  
##    10  0.238  0.815     0.170
##    20  0.244  0.811     0.175
##    30  0.252  0.802     0.181
##    40  0.261  0.793     0.188
##    50  0.270  0.783     0.194
##    60  0.279  0.773     0.201
##    70  0.288  0.763     0.207
##    80  0.297  0.753     0.213
##    90  0.306  0.741     0.220
##   100  0.315  0.729     0.226
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 10.</code></pre>
<pre class="r"><code># plot results
plot(knn_model2)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/K-NN%20more%20neighbors-2.png" width="648" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Given that the model that gives us the lowest RMSE is knn_model (instead of
the second one), we choose it as our final knn model.</p>
</blockquote>
<pre class="r"><code>predictions_knn &lt;- predict(knn_model, test_data)

knn_results&lt;-data.frame(RMSE = RMSE(predictions_knn, log(test_data$price)),
                         Rsquare = R2(predictions_knn, log(test_data$price)))

knn_results</code></pre>
<pre><code>##    RMSE Rsquare
## 1 0.233   0.809</code></pre>
<blockquote>
<p>The K-NN model is the best-performing one: the out-of-sample RMSE is 0.2328593
and the Rsquare is 0.8091817 (out-of-sample).</p>
</blockquote>
</div>
<div id="random-forest-model" class="section level1">
<h1>Random Forest model</h1>
<pre class="r"><code>library(ranger)

# Define control parameters for train function below
rf_control &lt;- trainControl(
    method=&quot;cv&quot;,
    number=5,
    savePredictions=&quot;final&quot;,
    summaryFunction=defaultSummary,
    verboseIter = TRUE
  )
  
# Define the tuning grid
gridRF &lt;- data.frame(
  .mtry = c(1:7), #range of number of variables randomly selected
  .splitrule = &quot;variance&quot;,
  .min.node.size = 5
)

# Fit random forest with ranger model
rf_model &lt;- train(
  log(price) ~ total_floor_area + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = &quot;ranger&quot;,
  metric=&quot;RMSE&quot;,
  trControl = rf_control,
  tuneGrid = gridRF,
  importance = &quot;permutation&quot; # used to estimate variable importance
)</code></pre>
<pre><code>## + Fold1: mtry=1, splitrule=variance, min.node.size=5 
## - Fold1: mtry=1, splitrule=variance, min.node.size=5 
## + Fold1: mtry=2, splitrule=variance, min.node.size=5 
## - Fold1: mtry=2, splitrule=variance, min.node.size=5 
## + Fold1: mtry=3, splitrule=variance, min.node.size=5 
## - Fold1: mtry=3, splitrule=variance, min.node.size=5 
## + Fold1: mtry=4, splitrule=variance, min.node.size=5 
## - Fold1: mtry=4, splitrule=variance, min.node.size=5 
## + Fold1: mtry=5, splitrule=variance, min.node.size=5 
## - Fold1: mtry=5, splitrule=variance, min.node.size=5 
## + Fold1: mtry=6, splitrule=variance, min.node.size=5 
## - Fold1: mtry=6, splitrule=variance, min.node.size=5 
## + Fold1: mtry=7, splitrule=variance, min.node.size=5 
## - Fold1: mtry=7, splitrule=variance, min.node.size=5 
## + Fold2: mtry=1, splitrule=variance, min.node.size=5 
## - Fold2: mtry=1, splitrule=variance, min.node.size=5 
## + Fold2: mtry=2, splitrule=variance, min.node.size=5 
## - Fold2: mtry=2, splitrule=variance, min.node.size=5 
## + Fold2: mtry=3, splitrule=variance, min.node.size=5 
## - Fold2: mtry=3, splitrule=variance, min.node.size=5 
## + Fold2: mtry=4, splitrule=variance, min.node.size=5 
## - Fold2: mtry=4, splitrule=variance, min.node.size=5 
## + Fold2: mtry=5, splitrule=variance, min.node.size=5 
## - Fold2: mtry=5, splitrule=variance, min.node.size=5 
## + Fold2: mtry=6, splitrule=variance, min.node.size=5 
## - Fold2: mtry=6, splitrule=variance, min.node.size=5 
## + Fold2: mtry=7, splitrule=variance, min.node.size=5 
## - Fold2: mtry=7, splitrule=variance, min.node.size=5 
## + Fold3: mtry=1, splitrule=variance, min.node.size=5 
## - Fold3: mtry=1, splitrule=variance, min.node.size=5 
## + Fold3: mtry=2, splitrule=variance, min.node.size=5 
## - Fold3: mtry=2, splitrule=variance, min.node.size=5 
## + Fold3: mtry=3, splitrule=variance, min.node.size=5 
## - Fold3: mtry=3, splitrule=variance, min.node.size=5 
## + Fold3: mtry=4, splitrule=variance, min.node.size=5 
## - Fold3: mtry=4, splitrule=variance, min.node.size=5 
## + Fold3: mtry=5, splitrule=variance, min.node.size=5 
## - Fold3: mtry=5, splitrule=variance, min.node.size=5 
## + Fold3: mtry=6, splitrule=variance, min.node.size=5 
## - Fold3: mtry=6, splitrule=variance, min.node.size=5 
## + Fold3: mtry=7, splitrule=variance, min.node.size=5 
## - Fold3: mtry=7, splitrule=variance, min.node.size=5 
## + Fold4: mtry=1, splitrule=variance, min.node.size=5 
## - Fold4: mtry=1, splitrule=variance, min.node.size=5 
## + Fold4: mtry=2, splitrule=variance, min.node.size=5 
## - Fold4: mtry=2, splitrule=variance, min.node.size=5 
## + Fold4: mtry=3, splitrule=variance, min.node.size=5 
## - Fold4: mtry=3, splitrule=variance, min.node.size=5 
## + Fold4: mtry=4, splitrule=variance, min.node.size=5 
## - Fold4: mtry=4, splitrule=variance, min.node.size=5 
## + Fold4: mtry=5, splitrule=variance, min.node.size=5 
## - Fold4: mtry=5, splitrule=variance, min.node.size=5 
## + Fold4: mtry=6, splitrule=variance, min.node.size=5 
## - Fold4: mtry=6, splitrule=variance, min.node.size=5 
## + Fold4: mtry=7, splitrule=variance, min.node.size=5 
## - Fold4: mtry=7, splitrule=variance, min.node.size=5 
## + Fold5: mtry=1, splitrule=variance, min.node.size=5 
## - Fold5: mtry=1, splitrule=variance, min.node.size=5 
## + Fold5: mtry=2, splitrule=variance, min.node.size=5 
## - Fold5: mtry=2, splitrule=variance, min.node.size=5 
## + Fold5: mtry=3, splitrule=variance, min.node.size=5 
## - Fold5: mtry=3, splitrule=variance, min.node.size=5 
## + Fold5: mtry=4, splitrule=variance, min.node.size=5 
## - Fold5: mtry=4, splitrule=variance, min.node.size=5 
## + Fold5: mtry=5, splitrule=variance, min.node.size=5 
## - Fold5: mtry=5, splitrule=variance, min.node.size=5 
## + Fold5: mtry=6, splitrule=variance, min.node.size=5 
## - Fold5: mtry=6, splitrule=variance, min.node.size=5 
## + Fold5: mtry=7, splitrule=variance, min.node.size=5 
## - Fold5: mtry=7, splitrule=variance, min.node.size=5 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 4, splitrule = variance, min.node.size = 5 on full training set</code></pre>
<pre class="r"><code># Print model to console
summary(rf_model)</code></pre>
<pre><code>##                           Length Class         Mode     
## predictions               10498  -none-        numeric  
## num.trees                     1  -none-        numeric  
## num.independent.variables     1  -none-        numeric  
## mtry                          1  -none-        numeric  
## min.node.size                 1  -none-        numeric  
## variable.importance          13  -none-        numeric  
## prediction.error              1  -none-        numeric  
## forest                        7  ranger.forest list     
## splitrule                     1  -none-        character
## treetype                      1  -none-        character
## r.squared                     1  -none-        numeric  
## call                          9  -none-        call     
## importance.mode               1  -none-        character
## num.samples                   1  -none-        numeric  
## replace                       1  -none-        logical  
## xNames                       13  -none-        character
## problemType                   1  -none-        character
## tuneValue                     3  data.frame    list     
## obsLevels                     1  -none-        logical  
## param                         1  -none-        list</code></pre>
<pre class="r"><code>print(rf_model)</code></pre>
<pre><code>## Random Forest 
## 
## 10498 samples
##     8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 8397, 8399, 8398, 8399, 8399 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE   Rsquared  MAE  
##   1     0.370  0.731     0.270
##   2     0.267  0.798     0.193
##   3     0.241  0.815     0.174
##   4     0.237  0.817     0.171
##   5     0.237  0.816     0.171
##   6     0.238  0.815     0.171
##   7     0.238  0.813     0.172
## 
## Tuning parameter &#39;splitrule&#39; was held constant at a value of variance
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 4, splitrule = variance
##  and min.node.size = 5.</code></pre>
<pre class="r"><code>#printing number of trees fitted
print(rf_model$finalModel$num.trees)</code></pre>
<pre><code>## [1] 500</code></pre>
<pre class="r"><code>#making predictions with RF model
rf_predictions &lt;- predict(rf_model, test_data)

rf_results&lt;-data.frame(RMSE = RMSE(rf_predictions, log(test_data$price)),
                         Rsquare = R2(rf_predictions, log(test_data$price)))

rf_results</code></pre>
<pre><code>##    RMSE Rsquare
## 1 0.233   0.809</code></pre>
<blockquote>
<p>So far, the Random Forest (ensemble method) performs roughly as well as K-NN.
In fact, the out-of-sample RMSE and Rsquare obtained are 0.2332081 and 0.8085883.
The best performing random forest is obtained randomly selecting 4 out of the 13
independent variables in each reiteration (to build each of the 500 trees).</p>
</blockquote>
</div>
<div id="gradient-boosting-machine-model" class="section level1">
<h1>Gradient Boosting Machine model</h1>
<pre class="r"><code>library(gbm)

# Setting control parameters for gbm train function
gbm_control &lt;- trainControl(
  method=&quot;cv&quot;,
    number=5,
    savePredictions=&quot;final&quot;,
    summaryFunction=defaultSummary,
    verboseIter = TRUE
)

# Setting the tuning grid - trying multiple hyper parameters values for tuning
gridGBM &lt;- expand.grid(interaction.depth = 8, n.trees = seq(200,500, by = 100),
                       shrinkage = seq(0.075, 0.2, by = 0.025),
                       n.minobsinnode = 10)

set.seed(101)

# Train for GBM
gbm_model &lt;- train(
  log(price) ~ total_floor_area + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = &quot;gbm&quot;,
  trControl = gbm_control,
  tuneGrid = gridGBM,
  metric = &quot;RMSE&quot;,
  verbose = FALSE
)</code></pre>
<pre><code>## + Fold1: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## Aggregating results
## Selecting tuning parameters
## Fitting n.trees = 500, interaction.depth = 8, shrinkage = 0.075, n.minobsinnode = 10 on full training set</code></pre>
<pre class="r"><code># Displaying the model
print(gbm_model)</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 10498 samples
##     8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 8398, 8399, 8398, 8399, 8398 
## Resampling results across tuning parameters:
## 
##   shrinkage  n.trees  RMSE   Rsquared  MAE  
##   0.075      200      0.235  0.818     0.171
##   0.075      300      0.234  0.820     0.170
##   0.075      400      0.233  0.822     0.169
##   0.075      500      0.233  0.822     0.169
##   0.100      200      0.235  0.819     0.170
##   0.100      300      0.234  0.820     0.170
##   0.100      400      0.233  0.821     0.169
##   0.100      500      0.233  0.822     0.169
##   0.125      200      0.235  0.819     0.170
##   0.125      300      0.234  0.820     0.169
##   0.125      400      0.234  0.821     0.169
##   0.125      500      0.234  0.820     0.170
##   0.150      200      0.235  0.819     0.170
##   0.150      300      0.234  0.820     0.169
##   0.150      400      0.234  0.820     0.169
##   0.150      500      0.235  0.818     0.170
##   0.175      200      0.236  0.817     0.171
##   0.175      300      0.236  0.818     0.171
##   0.175      400      0.236  0.817     0.171
##   0.175      500      0.236  0.817     0.172
##   0.200      200      0.235  0.818     0.171
##   0.200      300      0.235  0.818     0.171
##   0.200      400      0.236  0.817     0.171
##   0.200      500      0.237  0.816     0.172
## 
## Tuning parameter &#39;interaction.depth&#39; was held constant at a value of 8
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 500, interaction.depth =
##  8, shrinkage = 0.075 and n.minobsinnode = 10.</code></pre>
<pre class="r"><code>#making predictions with GBM model
gbm_predictions &lt;- predict(gbm_model, test_data)

gbm_results&lt;-data.frame(RMSE = RMSE(gbm_predictions, log(test_data$price)),
                         Rsquare = R2(gbm_predictions, log(test_data$price)))

gbm_results</code></pre>
<pre><code>##    RMSE Rsquare
## 1 0.231   0.811</code></pre>
<blockquote>
<p>The Gradient Boosting Machine is a sequential boosting algorithm. We include
it in our models since we want to stack it together with a parallel ensemble
algorithm and keep diversity in our stacking. The GBM model on its own already
performs quite well: it is the best performing algorithm among all, with an RMSE
of 0.2313716 and an Rsquare of 0.8114689 out-of-sample.</p>
</blockquote>
</div>
<div id="stacking" class="section level1">
<h1>Stacking</h1>
<p>Use stacking to ensemble your algorithms.</p>
<blockquote>
<p>Since the tree is the worst performing method among the five built and does
not add significant diversity to the stacking, we are not going to include it
in the final model. The model included in the stacking are the linear model, the
K-NN model, the Random Forest and the Gradient Boosting Machine.</p>
</blockquote>
<pre class="r"><code># Setting seed
set.seed(121)

# Adding all models in caretList
final_model &lt;- caretList(
   log(price) ~ total_floor_area + district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
   data = train_data,
   metric = &quot;RMSE&quot;,
   tuneList = list(
     
        # Random Forest parameters
        rft = caretModelSpec(method = &quot;ranger&quot;,
                             tuneGrid = gridRF,
                             importance = &quot;permutation&quot;),
        
        # Gradient Boosting Machine parameters
        gbm = caretModelSpec(method = &quot;gbm&quot;,
                             tuneGrid = gridGBM,
                             verbose = FALSE),
        
        # K-NN parameters
        knn = caretModelSpec(method = &quot;knn&quot;,
                             tuneLength = 10,
                             preProcess = c(&quot;center&quot;, &quot;scale&quot;)),
        
        # Linear Regression parameters
        lm = caretModelSpec(method = &quot;leapBackward&quot;,
                            tuneGrid = data.frame(nvmax = 37:45))),
   
   # Train settings
   trControl = trainControl(method=&quot;cv&quot;,
                            number=5,
                            savePredictions=&quot;final&quot;,
                            summaryFunction=defaultSummary,
                            verboseIter = TRUE))</code></pre>
<pre><code>## + Fold1: mtry=1, splitrule=variance, min.node.size=5 
## - Fold1: mtry=1, splitrule=variance, min.node.size=5 
## + Fold1: mtry=2, splitrule=variance, min.node.size=5 
## - Fold1: mtry=2, splitrule=variance, min.node.size=5 
## + Fold1: mtry=3, splitrule=variance, min.node.size=5 
## - Fold1: mtry=3, splitrule=variance, min.node.size=5 
## + Fold1: mtry=4, splitrule=variance, min.node.size=5 
## - Fold1: mtry=4, splitrule=variance, min.node.size=5 
## + Fold1: mtry=5, splitrule=variance, min.node.size=5 
## - Fold1: mtry=5, splitrule=variance, min.node.size=5 
## + Fold1: mtry=6, splitrule=variance, min.node.size=5 
## - Fold1: mtry=6, splitrule=variance, min.node.size=5 
## + Fold1: mtry=7, splitrule=variance, min.node.size=5 
## - Fold1: mtry=7, splitrule=variance, min.node.size=5 
## + Fold2: mtry=1, splitrule=variance, min.node.size=5 
## - Fold2: mtry=1, splitrule=variance, min.node.size=5 
## + Fold2: mtry=2, splitrule=variance, min.node.size=5 
## - Fold2: mtry=2, splitrule=variance, min.node.size=5 
## + Fold2: mtry=3, splitrule=variance, min.node.size=5 
## - Fold2: mtry=3, splitrule=variance, min.node.size=5 
## + Fold2: mtry=4, splitrule=variance, min.node.size=5 
## - Fold2: mtry=4, splitrule=variance, min.node.size=5 
## + Fold2: mtry=5, splitrule=variance, min.node.size=5 
## - Fold2: mtry=5, splitrule=variance, min.node.size=5 
## + Fold2: mtry=6, splitrule=variance, min.node.size=5 
## - Fold2: mtry=6, splitrule=variance, min.node.size=5 
## + Fold2: mtry=7, splitrule=variance, min.node.size=5 
## - Fold2: mtry=7, splitrule=variance, min.node.size=5 
## + Fold3: mtry=1, splitrule=variance, min.node.size=5 
## - Fold3: mtry=1, splitrule=variance, min.node.size=5 
## + Fold3: mtry=2, splitrule=variance, min.node.size=5 
## - Fold3: mtry=2, splitrule=variance, min.node.size=5 
## + Fold3: mtry=3, splitrule=variance, min.node.size=5 
## - Fold3: mtry=3, splitrule=variance, min.node.size=5 
## + Fold3: mtry=4, splitrule=variance, min.node.size=5 
## - Fold3: mtry=4, splitrule=variance, min.node.size=5 
## + Fold3: mtry=5, splitrule=variance, min.node.size=5 
## - Fold3: mtry=5, splitrule=variance, min.node.size=5 
## + Fold3: mtry=6, splitrule=variance, min.node.size=5 
## - Fold3: mtry=6, splitrule=variance, min.node.size=5 
## + Fold3: mtry=7, splitrule=variance, min.node.size=5 
## - Fold3: mtry=7, splitrule=variance, min.node.size=5 
## + Fold4: mtry=1, splitrule=variance, min.node.size=5 
## - Fold4: mtry=1, splitrule=variance, min.node.size=5 
## + Fold4: mtry=2, splitrule=variance, min.node.size=5 
## - Fold4: mtry=2, splitrule=variance, min.node.size=5 
## + Fold4: mtry=3, splitrule=variance, min.node.size=5 
## - Fold4: mtry=3, splitrule=variance, min.node.size=5 
## + Fold4: mtry=4, splitrule=variance, min.node.size=5 
## - Fold4: mtry=4, splitrule=variance, min.node.size=5 
## + Fold4: mtry=5, splitrule=variance, min.node.size=5 
## - Fold4: mtry=5, splitrule=variance, min.node.size=5 
## + Fold4: mtry=6, splitrule=variance, min.node.size=5 
## - Fold4: mtry=6, splitrule=variance, min.node.size=5 
## + Fold4: mtry=7, splitrule=variance, min.node.size=5 
## - Fold4: mtry=7, splitrule=variance, min.node.size=5 
## + Fold5: mtry=1, splitrule=variance, min.node.size=5 
## - Fold5: mtry=1, splitrule=variance, min.node.size=5 
## + Fold5: mtry=2, splitrule=variance, min.node.size=5 
## - Fold5: mtry=2, splitrule=variance, min.node.size=5 
## + Fold5: mtry=3, splitrule=variance, min.node.size=5 
## - Fold5: mtry=3, splitrule=variance, min.node.size=5 
## + Fold5: mtry=4, splitrule=variance, min.node.size=5 
## - Fold5: mtry=4, splitrule=variance, min.node.size=5 
## + Fold5: mtry=5, splitrule=variance, min.node.size=5 
## - Fold5: mtry=5, splitrule=variance, min.node.size=5 
## + Fold5: mtry=6, splitrule=variance, min.node.size=5 
## - Fold5: mtry=6, splitrule=variance, min.node.size=5 
## + Fold5: mtry=7, splitrule=variance, min.node.size=5 
## - Fold5: mtry=7, splitrule=variance, min.node.size=5 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 7, splitrule = variance, min.node.size = 5 on full training set
## + Fold1: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold1: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold1: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold2: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold2: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold3: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold3: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold4: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold4: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.075, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.100, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.125, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.150, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.175, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## + Fold5: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## - Fold5: shrinkage=0.200, interaction.depth=8, n.minobsinnode=10, n.trees=500 
## Aggregating results
## Selecting tuning parameters
## Fitting n.trees = 300, interaction.depth = 8, shrinkage = 0.1, n.minobsinnode = 10 on full training set
## + Fold1: k= 5 
## - Fold1: k= 5 
## + Fold1: k= 7 
## - Fold1: k= 7 
## + Fold1: k= 9 
## - Fold1: k= 9 
## + Fold1: k=11 
## - Fold1: k=11 
## + Fold1: k=13 
## - Fold1: k=13 
## + Fold1: k=15 
## - Fold1: k=15 
## + Fold1: k=17 
## - Fold1: k=17 
## + Fold1: k=19 
## - Fold1: k=19 
## + Fold1: k=21 
## - Fold1: k=21 
## + Fold1: k=23 
## - Fold1: k=23 
## + Fold2: k= 5 
## - Fold2: k= 5 
## + Fold2: k= 7 
## - Fold2: k= 7 
## + Fold2: k= 9 
## - Fold2: k= 9 
## + Fold2: k=11 
## - Fold2: k=11 
## + Fold2: k=13 
## - Fold2: k=13 
## + Fold2: k=15 
## - Fold2: k=15 
## + Fold2: k=17 
## - Fold2: k=17 
## + Fold2: k=19 
## - Fold2: k=19 
## + Fold2: k=21 
## - Fold2: k=21 
## + Fold2: k=23 
## - Fold2: k=23 
## + Fold3: k= 5 
## - Fold3: k= 5 
## + Fold3: k= 7 
## - Fold3: k= 7 
## + Fold3: k= 9 
## - Fold3: k= 9 
## + Fold3: k=11 
## - Fold3: k=11 
## + Fold3: k=13 
## - Fold3: k=13 
## + Fold3: k=15 
## - Fold3: k=15 
## + Fold3: k=17 
## - Fold3: k=17 
## + Fold3: k=19 
## - Fold3: k=19 
## + Fold3: k=21 
## - Fold3: k=21 
## + Fold3: k=23 
## - Fold3: k=23 
## + Fold4: k= 5 
## - Fold4: k= 5 
## + Fold4: k= 7 
## - Fold4: k= 7 
## + Fold4: k= 9 
## - Fold4: k= 9 
## + Fold4: k=11 
## - Fold4: k=11 
## + Fold4: k=13 
## - Fold4: k=13 
## + Fold4: k=15 
## - Fold4: k=15 
## + Fold4: k=17 
## - Fold4: k=17 
## + Fold4: k=19 
## - Fold4: k=19 
## + Fold4: k=21 
## - Fold4: k=21 
## + Fold4: k=23 
## - Fold4: k=23 
## + Fold5: k= 5 
## - Fold5: k= 5 
## + Fold5: k= 7 
## - Fold5: k= 7 
## + Fold5: k= 9 
## - Fold5: k= 9 
## + Fold5: k=11 
## - Fold5: k=11 
## + Fold5: k=13 
## - Fold5: k=13 
## + Fold5: k=15 
## - Fold5: k=15 
## + Fold5: k=17 
## - Fold5: k=17 
## + Fold5: k=19 
## - Fold5: k=19 
## + Fold5: k=21 
## - Fold5: k=21 
## + Fold5: k=23 
## - Fold5: k=23 
## Aggregating results
## Selecting tuning parameters
## Fitting k = 7 on full training set
## + Fold1: nvmax=45 
## - Fold1: nvmax=45 
## + Fold2: nvmax=45 
## - Fold2: nvmax=45 
## + Fold3: nvmax=45 
## - Fold3: nvmax=45 
## + Fold4: nvmax=45 
## - Fold4: nvmax=45 
## + Fold5: nvmax=45 
## - Fold5: nvmax=45 
## Aggregating results
## Selecting tuning parameters
## Fitting nvmax = 45 on full training set</code></pre>
<pre class="r"><code>#Stacking all final models
glm_ensemble &lt;- caretEnsemble::caretStack(final_model,
                                          method = &quot;glm&quot;,
                                          metric = &quot;RMSE&quot;,
                                          trControl = trainControl(&quot;cv&quot;,
                                                                   10))

#Analyzing performance of the model
glm_ensemble$error</code></pre>
<pre><code>##   parameter  RMSE Rsquared   MAE RMSESD RsquaredSD   MAESD
## 1      none 0.212    0.852 0.152 0.0067    0.00963 0.00424</code></pre>
<pre class="r"><code>fm_predictions &lt;- predict(glm_ensemble, test_data)

resamples &lt;- resamples(final_model)

dotplot(resamples, metric = &quot;RMSE&quot;)</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/final%20stacking-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>summary(glm_ensemble)</code></pre>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.667  -0.106   0.008   0.122   1.215  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.5187     0.0649   -7.99  1.5e-15 ***
## rft           0.2143     0.0320    6.69  2.3e-11 ***
## gbm           0.6115     0.0275   22.23  &lt; 2e-16 ***
## knn           0.1667     0.0156   10.72  &lt; 2e-16 ***
## lm            0.0472     0.0186    2.53    0.011 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.045)
## 
##     Null deviance: 3191.1  on 10497  degrees of freedom
## Residual deviance:  472.6  on 10493  degrees of freedom
## AIC: -2747
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<pre class="r"><code># Plotting all KPI of all models used in stacking
resamples(list(
  LM=final_model$lm,
  KNN = final_model$knn,
  RFOREST = final_model$rft,
  GBOOSTING=final_model$gbm)) %&gt;%
  bwplot(scales=list(x=list(relation=&#39;free&#39;), y=list(relation=&#39;free&#39;)))</code></pre>
<p><img src="/blogs/ensemble_algorithms_files/figure-html/final%20stacking-2.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="pick-investments" class="section level1">
<h1>Pick investments</h1>
<p>In this section you should use the best algorithm you identified to choose 200 properties from the out of sample data.</p>
<blockquote>
<p>The best performing algorithm is the stacking, with an RMSE of 0.2122337 and
an Rsquared of 0.8518665.</p>
</blockquote>
<pre class="r"><code>numchoose=200

oos&lt;-london_house_prices_2019_out_of_sample

#predict the value of houses
oos$predict &lt;- exp(predict(glm_ensemble, oos))

#Choose the ones you want to invest here
oos &lt;- oos %&gt;%
  mutate(profit_pct = ((predict-asking_price)/asking_price)*100) %&gt;%
  arrange(desc(profit_pct)) %&gt;%
  mutate(buy = rep(c(1,0),c(numchoose,(nrow(oos)-numchoose))))

#output your choices. Change the name of the file to your &quot;lastname_firstname.csv&quot;
#write.csv(oos,&quot;nicoli-francesco.csv&quot;)</code></pre>
</div>
