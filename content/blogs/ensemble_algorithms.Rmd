---
title: "Predicting London House Prices"
date: "2022-09-18"
description: Predicting London House Prices using Ensemble algorithms with R
draft: no
image: ensemble_algos.jpg
keywords: ''
slug: ensemble_algorithms
categories:
- ''
- ''
---
```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)
# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>



```{r, load_libraries, include = FALSE, message = FALSE, warning=FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse", repos = "http://cran.us.r-project.org")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc", repos = "http://cran.us.r-project.org")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2", repos = "http://cran.us.r-project.org")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes", repos = "http://cran.us.r-project.org")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor", repos = "http://cran.us.r-project.org")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot", repos = "http://cran.us.r-project.org")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
library(caretEnsemble)
library(leaps)
```

# Introduction and learning objectives

<div class = "navy1">
The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.


<b>Learning objectives</b>
 
<ol type="i">
  <li>Using different data mining algorithms for prediction.</li>
  <li>Dealing with large data sets</li>
  <li>Tuning data mining algorithms</li>
  <li>Interpreting data mining algorithms and deducing importance of variables</li>
  <li>Using results of data mining algorithms to make business decisions</li>
</ol>  
</div>

# Load data

There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets. 

Make sure you understand what information each column contains. Note that not all information provided might be useful in predicting house prices, but do not make any assumptions before you decide what information you use in your prediction algorithms.

```{r read-investigate, warning=FALSE, message=FALSE}
#read in the data

london_house_prices_2019_training<-read.csv("C:/Users/optif/Desktop/academics/applied_statistics/Francesco_repository_1/data/training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample<-read.csv("C:/Users/optif/Desktop/academics/applied_statistics/Francesco_repository_1/data/test_data_assignment.csv")

#fix data types in both data sets

#fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)

#take a quick look at what's in the data
str(london_house_prices_2019_training)
str(london_house_prices_2019_out_of_sample)



```


```{r train-test split, warning = FALSE, message = FALSE}
#dropping columns with missing values
london_house_prices_2019_training <- london_house_prices_2019_training %>%
  select(-c(address2, town, population))

#let's do the initial split
library(rsample)
set.seed(123)
train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset
train_data <- training(train_test_split)
test_data <- testing(train_test_split)

```


# Visualize data 

Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?

```{r visualize, message = FALSE, warning=FALSE}

# Analyzing the dataset
skimr::skim(london_house_prices_2019_training)

# Let's first see how the response variable's distribution looks like in the
#training data
ggplot(london_house_prices_2019_training, aes(x=log(price))) +
  geom_density()

# Visualizing distribution of numerical variables highly correlated with price
# distance_to_station, highly skewed variable (-0.16)
ggplot(london_house_prices_2019_training, aes(x=distance_to_station)) +
  geom_density()

ggplot(london_house_prices_2019_training, aes(x=distance_to_station, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

# average_income (0.42)
ggplot(london_house_prices_2019_training, aes(x=average_income)) +
  geom_density()

ggplot(london_house_prices_2019_training, aes(x=average_income, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

# co2_emissions_current (0.58)
ggplot(london_house_prices_2019_training, aes(x=co2_emissions_current)) +
  geom_density()

ggplot(london_house_prices_2019_training, aes(x=co2_emissions_potential, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

# total_floor_area (0.74, highly correlated with co2_emissions_potential)
ggplot(london_house_prices_2019_training, aes(x=total_floor_area)) +
  geom_density()

ggplot(london_house_prices_2019_training, aes(x=total_floor_area, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

# Visualizing relationship with qualitative variables
# london_zone (moderate correlation)
london_house_prices_2019_training %>%
  group_by(london_zone) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=london_zone,
             y=median_price,
             fill=london_zone)) +
  geom_bar(stat = 'identity')

# property_type (moderate correlation)
london_house_prices_2019_training %>%
  group_by(property_type) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=property_type,
             y=median_price,
             fill=property_type)) +
  geom_bar(stat = 'identity')

# freehold_or_leasehold (moderate correlation)
london_house_prices_2019_training %>%
  group_by(freehold_or_leasehold) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=freehold_or_leasehold,
             y=median_price,
             fill=freehold_or_leasehold)) +
  geom_bar(stat = 'identity')

# current_energy_rating (moderate correlation - high cor with co2_emissions)
london_house_prices_2019_training %>%
  group_by(current_energy_rating) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=current_energy_rating,
             y=median_price,
             fill=current_energy_rating)) +
  geom_bar(stat = 'identity')

# tenure (weak correlation)
london_house_prices_2019_training %>%
  group_by(tenure) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=tenure,
             y=median_price,
             fill=tenure)) +
  geom_bar(stat = 'identity')

# water_company (moderate to high correlation)
london_house_prices_2019_training %>%
  group_by(water_company) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=water_company,
             y=median_price,
             fill=water_company)) +
  geom_bar(stat = 'identity')

# type_of_closest_station (moderate to high correlation)
london_house_prices_2019_training %>%
  group_by(type_of_closest_station) %>%
  summarise(median_price = mean(price)) %>%
  ggplot(aes(x=type_of_closest_station,
             y=median_price,
             fill=type_of_closest_station)) +
  geom_bar(stat = 'identity')

```
> First, it is important to skimr::skim() the data set in order to get a rough
sense of the main features of the data: missing data, minimum, maximum, rough
representation of the distribution. In addition, it is crucial to visualise the
distribution of continuous variables (especially those most correlated with the
output variable we are trying to predict), in order to get a sense of their
skewness and correlation with the price. In fact, we do the same with categorical
variables: we visualize how those variables affect the price.

> Through these visualisations, we can learn the type and magnitude of the
correlation with the output variable for each input variable and choose which to
include in our models accordingly.


Estimate a correlation table between prices and other continuous variables. What do you glean from the correlation table?

```{r, correlation table, warning=FALSE, message=FALSE}

# produce a correlation table using GGally::ggcor()
# this takes a while to plot

library("GGally")
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  mutate(price = log(price)) %>%
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2,angle = -20)

```
> The correlation table is a crucial tool when it comes to build predictive
models: it allows to get a sense of the magnitude of the correlation among all
the numerical variables in the dataset. In this case, we can see how price is highly
correlated with average_income, num_tube_lines, london_zone (better if read as a
factor), co2_emissions_potential, co2_emissions_current, number_habitable_rooms
and total_floor_area.

> It is also possible to get a sense of the correlation among regressors: the
correlation table is extremely useful when it comes to assessing the potential
of multicolinearity in the model, due to correlation among predictors.

# Fit a linear regression model

To help you get started I build a linear regression model below. I chose a subset of the features with no particular goal. You can (and should) add more variables and/or choose variable selection methods if you want.

```{r LR step-wise model, message=FALSE, warning=FALSE}

#Define control variables
control <- trainControl(
    method="cv",
    number=5,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

set.seed(987)
#We are going to train the model on the training data and select the features
#using the stepwise selection method
lm_model <- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = "leapBackward",
  tuneGrid = data.frame(nvmax = 67:77),
  trControl = control
)

#linear check
linear <- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = "lm",
  trControl = control
)

#shows results of all models
lm_model$results

#summarizes model of best fit's coefficients
coef(lm_model$finalModel, lm_model$bestTune$nvmax)

```


```{r variable importance check, message = FALSE, warning=FALSE}
# we can check variable importance as well
importance <- varImp(lm_model, scale=TRUE)
plot(importance)

```

## Predict the values in testing and out of sample data

Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?

```{r predictions test, warning = FALSE, message=FALSE}
#we can predict the testing values
predictions <- predict(lm_model,test_data)

lr_results<-data.frame(RMSE = RMSE(predictions, log(test_data$price)),
                       Rsquare = R2(predictions, log(test_data$price)))

lr_results

```

> The quality of a linear regression model can be measured by the RMSE and the
Rsquare. The RMSE (Root Mean Squared Error) measures how much do the predictions
of the model differ from the actual output values. The R-squared measures what
percentage of the variance in the output variable is explained by the model. In
orderly to properly test the quality of our predictions, it's crucial to compute
the RMSE and R2 indices on the testing data before computing them on the testing
data in order to evaluate if the model tends to overfit the training data.

# Fit a tree model

Next I fit a tree model using the same subset of features. Again you can (and should) add more variables and tune the parameter of your tree to find a better fit. 

Compare the performance of the linear regression model with the tree model; which one performs better? Why do you think that is the case?

```{r tree model, warning = FALSE, message = FALSE}

#Tuning the model
tree_model <- train(
  log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = "rpart",
  trControl = control,
  tuneLength=20
    )

#You can view how the tree performs
tree_model$results

#You can view the final tree
rpart.plot(tree_model$finalModel)

```

```{r tree performance, warning = FALSE, message = FALSE}

# We can predict the testing values
predictions_tree <- predict(tree_model, test_data)

tree_results<-data.frame(RMSE = RMSE(predictions_tree, log(test_data$price)),
                         Rsquare = R2(predictions_tree, log(test_data$price)))

tree_results

```

> Assuming that the response variable is the logarithm of the price (instead of
the price itself), we can see that the linear regression performs significantly
better than the tree algorithm. In fact, the RMSE and Rsquare for linear
regression are 0.2359316 and 0.8038051 respectively, whereas the same indices
for the tree model are 0.2985389 and 0.6863361 respectively.

> Tree algorithms usually perform better than linear regressions when the
relationship between the predictor variables and the response variable is
non-linear. However, when the relationship between response and predictor
variables is at least approximately linear, the linear regression performs
better. This happens because a linear increase in the predicted value - caused
by a linear increase in the predictors - allows for a lower error than the one
obtained when the predicted value stays the same even if predictors change,
albeit the change is within a certain margin (that is the margin defined by the
different splits in the model).

> In addition, in order to increase the performance of trees, we could increase
the number of levels. However, this would make the model difficult to interpret.

# K-NN model

```{r K-NN initial fit, warning = FALSE, message = FALSE}

set.seed(456)

#fitting the model
knn_model <- train(log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
                   train_data,
                   method = "knn",
                   trControl = trainControl("cv", number = 10), #use 10 fold cross validation
                   tuneLength = 10, #number of parameter values tried by function
                   preProcess = c("center", "scale"))

#showing model results
knn_model
plot(knn_model)

#visualising the importance of each variable in knn_model
importance_knn <- varImp(knn_model, scale=TRUE)
plot(importance_knn)

```

```{r K-NN more neighbors, warning = FALSE, message = FALSE}

#fit knn with expand.grid including more neighbors
knnGrid <-  expand.grid(k= seq(10,100 , by = 10)) 

set.seed(789)

#the metric used to choose k is the default "RMSE"
knn_model2 <- train(log(price) ~ total_floor_area*district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
                    train_data,
                    preProcess = c("center", "scale"),
                    method="knn",
                    trControl=trainControl("cv", number = 10),
                    tuneGrid = knnGrid)

#display importance of variables
importance_knn2 <- varImp(knn_model2, scale=TRUE)
plot(importance_knn2)

# display results
print(knn_model2)
# plot results
plot(knn_model2)

```

> Given that the model that gives us the lowest RMSE is knn_model (instead of
the second one), we choose it as our final knn model.

```{r K-NN prediction, warning = FALSE, message = FALSE}

predictions_knn <- predict(knn_model, test_data)

knn_results<-data.frame(RMSE = RMSE(predictions_knn, log(test_data$price)),
                         Rsquare = R2(predictions_knn, log(test_data$price)))

knn_results

```

> The K-NN model is the best-performing one: the out-of-sample RMSE is 0.2328593
and the Rsquare is 0.8091817 (out-of-sample).

# Random Forest model

```{r Random Forest fit, warning = FALSE, message = FALSE}

library(ranger)

# Define control parameters for train function below
rf_control <- trainControl(
    method="cv",
    number=5,
    savePredictions="final",
    summaryFunction=defaultSummary,
    verboseIter = TRUE
  )
  
# Define the tuning grid
gridRF <- data.frame(
  .mtry = c(1:7), #range of number of variables randomly selected
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest with ranger model
rf_model <- train(
  log(price) ~ total_floor_area + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = "ranger",
  metric="RMSE",
  trControl = rf_control,
  tuneGrid = gridRF,
  importance = "permutation" # used to estimate variable importance
)

# Print model to console
summary(rf_model)
print(rf_model)

#printing number of trees fitted
print(rf_model$finalModel$num.trees)

```

```{r Random Forest prediction, warning = FALSE, message = FALSE}

#making predictions with RF model
rf_predictions <- predict(rf_model, test_data)

rf_results<-data.frame(RMSE = RMSE(rf_predictions, log(test_data$price)),
                         Rsquare = R2(rf_predictions, log(test_data$price)))

rf_results

```

> So far, the Random Forest (ensemble method) performs roughly as well as K-NN.
In fact, the out-of-sample RMSE and Rsquare obtained are 0.2332081 and 0.8085883.
The best performing random forest is obtained randomly selecting 4 out of the 13
independent variables in each reiteration (to build each of the 500 trees).

# Gradient Boosting Machine model

```{r Gradient Boosting Machine initial fit, warning = FALSE, message = FALSE}

library(gbm)

# Setting control parameters for gbm train function
gbm_control <- trainControl(
  method="cv",
    number=5,
    savePredictions="final",
    summaryFunction=defaultSummary,
    verboseIter = TRUE
)

# Setting the tuning grid - trying multiple hyper parameters values for tuning
gridGBM <- expand.grid(interaction.depth = 8, n.trees = seq(200,500, by = 100),
                       shrinkage = seq(0.075, 0.2, by = 0.025),
                       n.minobsinnode = 10)

set.seed(101)

# Train for GBM
gbm_model <- train(
  log(price) ~ total_floor_area + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
  train_data,
  method = "gbm",
  trControl = gbm_control,
  tuneGrid = gridGBM,
  metric = "RMSE",
  verbose = FALSE
)

# Displaying the model
print(gbm_model)

```

```{r Gradient Boosting Machine predictions, warning = FALSE, message = FALSE}

#making predictions with GBM model
gbm_predictions <- predict(gbm_model, test_data)

gbm_results<-data.frame(RMSE = RMSE(gbm_predictions, log(test_data$price)),
                         Rsquare = R2(gbm_predictions, log(test_data$price)))

gbm_results


```

> The Gradient Boosting Machine is a sequential boosting algorithm. We include
it in our models since we want to stack it together with a parallel ensemble
algorithm and keep diversity in our stacking. The GBM model on its own already
performs quite well: it is the best performing algorithm among all, with an RMSE
of 0.2313716 and an Rsquare of 0.8114689 out-of-sample.

# Stacking

Use stacking to ensemble your algorithms.

> Since the tree is the worst performing method among the five built and does
not add significant diversity to the stacking, we are not going to include it
in the final model. The model included in the stacking are the linear model, the
K-NN model, the Random Forest and the Gradient Boosting Machine.

```{r final stacking, warning=FALSE, message=FALSE }

# Setting seed
set.seed(121)

# Adding all models in caretList
final_model <- caretList(
   log(price) ~ total_floor_area + district + water_company + property_type + london_zone + num_tube_lines + co2_emissions_current + number_habitable_rooms + average_income,
   data = train_data,
   metric = "RMSE",
   tuneList = list(
     
        # Random Forest parameters
        rft = caretModelSpec(method = "ranger",
                             tuneGrid = gridRF,
                             importance = "permutation"),
        
        # Gradient Boosting Machine parameters
        gbm = caretModelSpec(method = "gbm",
                             tuneGrid = gridGBM,
                             verbose = FALSE),
        
        # K-NN parameters
        knn = caretModelSpec(method = "knn",
                             tuneLength = 10,
                             preProcess = c("center", "scale")),
        
        # Linear Regression parameters
        lm = caretModelSpec(method = "leapBackward",
                            tuneGrid = data.frame(nvmax = 37:45))),
   
   # Train settings
   trControl = trainControl(method="cv",
                            number=5,
                            savePredictions="final",
                            summaryFunction=defaultSummary,
                            verboseIter = TRUE))

#Stacking all final models
glm_ensemble <- caretEnsemble::caretStack(final_model,
                                          method = "glm",
                                          metric = "RMSE",
                                          trControl = trainControl("cv",
                                                                   10))

#Analyzing performance of the model
glm_ensemble$error

fm_predictions <- predict(glm_ensemble, test_data)

resamples <- resamples(final_model)

dotplot(resamples, metric = "RMSE")

summary(glm_ensemble)

# Plotting all KPI of all models used in stacking
resamples(list(
  LM=final_model$lm,
  KNN = final_model$knn,
  RFOREST = final_model$rft,
  GBOOSTING=final_model$gbm)) %>%
  bwplot(scales=list(x=list(relation='free'), y=list(relation='free')))

```


# Pick investments

In this section you should use the best algorithm you identified to choose 200 properties from the out of sample data.

> The best performing algorithm is the stacking, with an RMSE of 0.2122337 and
an Rsquared of 0.8518665.

```{r,warning=FALSE,  message=FALSE }

numchoose=200

oos<-london_house_prices_2019_out_of_sample

#predict the value of houses
oos$predict <- exp(predict(glm_ensemble, oos))

#Choose the ones you want to invest here
oos <- oos %>%
  mutate(profit_pct = ((predict-asking_price)/asking_price)*100) %>%
  arrange(desc(profit_pct)) %>%
  mutate(buy = rep(c(1,0),c(numchoose,(nrow(oos)-numchoose))))

#output your choices. Change the name of the file to your "lastname_firstname.csv"
#write.csv(oos,"nicoli-francesco.csv")

```

