---
title: "Text Mining Analysis & Neural Network fit on Movie Data"
date: "2022-09-18"
description: Text Mining Analysis & Neural Network fit on Movie Data
draft: no
image: movie_banner.jpg
keywords: ''
slug: ml_final_project
categories:
- ''
- ''
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)
# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

# Part 1: Text Mining Analysis

```{r package_loading, include=FALSE}
library(knitr)
library(tidyverse)
library(tm) # package for text mining  
library(SnowballC) # for stemming words
library(stringr) # package to count number of words in a string
library(RWeka) # package for ngrams
library(data.table) # for reading datasets faster
library(purrr)
```


## Data Preprocessing

Before taking on the Text Mining Analysis we intend to perform on the Movie
Data, we need to preprocess the **tags dataset**: the one containing the tags
from each user regarding each movie. The preprocessing consists of removing
duplicates, merging the tags in one line and removing special characters.

```{r Read Dataset, message=FALSE, warning=FALSE}

# Reading tags.csv file and keeping "movieId" and "tags" columns
tags <- fread("C:/Users/optif/OneDrive/Desktop/academics/applied_statistics/Francesco_repository_1/data/tags-file2.csv")
tags <- tags %>%
  select(movieId, tag)

dim(tags) # print dimensions
```

```{r Remove Duplicates, message=FALSE, warning=FALSE}

# Removing duplicated movieTags and aggregating tags
tb <- tags %>%
  group_by(movieId) %>%
  summarise(tag = paste(unlist(list(as.character(tag))), collapse = " "))

dim(tb) # print dimensions
```

```{r Filter for long strings, message=FALSE, warning=FALSE}

# Filter for tags with at least 100 words
tb <- tb %>%
  filter(as.numeric(sapply(strsplit(tag, " "), length)) > 100)

dim(tb) # print dimensions
```

```{r Remove special characters, message=FALSE, warning=FALSE}
# Removing special characters from tags
tb <- tb %>%
  mutate(tag = gsub("[[:punct:]]", "", tag))

dim(tb) # print dimensions
```

Our task is to build a **Document Term Matrix** containing individual movies as
documents and terms/words occurring in tags as columns. Furthermore, we decided
to create **bi_grams** in our dataset. Hence, we have to be careful as our
matrix will become sparse very quickly.

```{r Document-Term Matrix, message = FALSE, warning = FALSE}

# Tags corpus
corp_tags <- VCorpus(VectorSource(tb$tag))
inspect(corp_tags[1:5]) # inspecting first 5 rows of corpus

# Creating the BigramTokenizer function for creating Bi-grams in the DTM
BigramTokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=1, max=2))
}

corp_tags <- tm_map(corp_tags, stripWhitespace)

# Converting corp_tags into DocumentTermMatrix object
DTM_tags_DF_IDF <- DocumentTermMatrix(corp_tags, control = list(
            tolower = TRUE, # Converts to lowercase
            removeNumbers = TRUE, # Removes numbers
            stopwords = TRUE, # Removes common words with almost 0 info value
            removePunctuation = TRUE, # Even if we did it previously
            stripWhitespace = TRUE, # Removes white space characters
            tokenize = BigramTokenizer, # Setting Tokenizer parameter
            weighting = weightTfIdf)) # Setting weighting to Inverse Document
                                      # Frequency

dim(DTM_tags_DF_IDF) # print dimensions of DTM_tags_DF_IDF

inspect(DTM_tags_DF_IDF[,c(1:5)]) # inspecting content of first 5
                                            # columns of DTM_tags_DF_IDF

```

In this first step of the Text Mining phase, we decided to create a
Document-Term Matrix by converting all characters to *lowercase* characters,
removing all *numbers*, removing *common words* (since those words don't provide
informational value), removing *punctuation* and stripping additional white spaces
in the strings. We also decided to add **bi-grams** to the DTM in order to
include an ad-hoc analysis of the many two-word tags coming in the dataset.

In addition, we created the DTM object that stores the number of occurrences
computed using the **Term Frequency - Inverse Document Frequency** adjustment
rule. This method allows us to take into account both the frequency of terms in
a tag and the frequency of tags containing that term, thus identifying documents
with frequent occurrences of rare terms.

In the next step, we address sparsity for both single words and bi-grams.

```{r Address Sparsity, message = FALSE, warning = FALSE}

# Removing sparse tokens from the DTM_tags_DF_IDF
DTM_tags_TF_IDF2 <- removeSparseTerms(DTM_tags_DF_IDF, 0.996) # 99.6% maximum
                                                              # sparsity

# Checking for dimensions of new DTM
dim(DTM_tags_TF_IDF2)

# Printing the first five terms of the new DTM
inspect(DTM_tags_TF_IDF2[,c(1:5)])

# Return the number of bi-grams in the DTM_tags_TF_IDF2
sum(as.numeric(sapply(strsplit(trimws(DTM_tags_TF_IDF2$dimnames$Terms), " "),
                      length)) > 1)

```

In this last step of the Text Mining section, we filtered for terms with a
*sparsity* less or equal to **99.6%**. This value seems reasonable since 0.4%
of the tags is around 12 tags: we definitely do not want to include tokens
appearing in less than 12 tags but tokens that appear in more than 12 tags can
turn out to be useful for the analysis. In conclusion, we compute the number of
bi-grams present in our Document Term Matrix: **2839** bi-grams out of 6060 words
filtered.

# Principle Component Analysis

Now that we have a DTM, we can use it in an **unsupervised machine learning**
**algorithm** that can reduce the dimensionality of the data. 
Specifically we have terms/words that describe each movie, however likely
we have way too many columns and should only use a **reduced amount of columns**
in our further analysis. For example we may wish to run a classification
algorithm such as an SVM as a final step in order to be able to create a model 
that can predict a movie's rating based on some features, including the features
produced as a result of running the PCA.

Therefore our next task is to run the **PCA** on the Document Term Matrix that
we designed above. As a result of the PCA we should provide the PC
coordinates/scores to be used as features in Part 3. Crucially, we must decide
on the number of these new columns (containing the PC scores) that should be
used.

```{r PCA Data Preprocessing, message=FALSE, warning=FALSE}

# Converting the DTM object into a tibble first
m <- as.matrix(DTM_tags_TF_IDF2)
DTM_tags_tbl <- as_tibble(m)

# Standardizing the dataset
DTM_pca <- scale(DTM_tags_tbl, center = TRUE, scale = TRUE)

# Running PCA on DTM_pca
tags_pca <- prcomp(DTM_pca, center = FALSE, scale. = FALSE)
```

After *standardizing* the dataset and running the *PCA* on it, we are ready to
perform the **Analysis of the Variance**.

```{r Analysis of Variance, message=FALSE, warning=FALSE}

# "Variance Explained" as vector of the SDev^2 explained by each PC
VE <- tags_pca$sdev^2

# "Percentage of Variance Explained" as proportion of total variance explained
# by each PC
PVE <- VE/sum(VE)*100

# Cumulative Percentage of Variance Explained
CPVE <- cumsum(PVE)

# Creating Data frame object containing data about Variance Explained by each PC
df <- data.frame(PC = c(1:length(tags_pca$sdev)), 
                 var_explained = VE,
                 cum_sum_PVE = CPVE)

# Building Scree Plot of the Variance Explained
df %>%
  ggplot(aes(x = PC, y = var_explained)) +
  geom_point(size = 4) +
  geom_line() + 
  scale_x_continuous(breaks = seq(0,3000,250)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", size = 1.2) +
  labs(x = "PC Number", y = "VE", 
       title = "Scree Plot", 
       subtitle = "PCA on tags Data") +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold", color = "blue"),
        plot.title = element_text(size = 18, color = "blue"),
        plot.subtitle = element_text(size = 15, color = "blue")
  )
```

Since it is difficult to pinpoint where the elbow occurs exactly, we are going
to build another Scree Plot showing the Variance Explained by all the PCs from
the 1st to the 50th.

```{r Analysis of Variance Visualisation, message=FALSE, warning=FALSE}

# Building Scree Plot of the Variance Explained for the first 50 PCs
df[c(1:50),] %>%
  ggplot(aes(x = PC, y = var_explained)) +
  geom_point(size = 4) +
  geom_line() + 
  scale_x_continuous(breaks = seq(0,50,5)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", size = 1.2) +
  labs(x = "PC Number", y = "VE", 
       title = "Scree Plot", 
       subtitle = "PCA on Tags Data") +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold", color = "blue"),
        plot.title = element_text(size = 18, color = "blue"),
        plot.subtitle = element_text(size = 15, color = "blue")
  )
```

As we can see by the Scree Plot, the elbow occurs at the 10th PC. In fact, after
this PC, the amount of variance explained tends to flatten out. Hence, we are
going to keep **10 PCs** for our further analysis.

```{r Saving 10 PCs, message=FALSE, warning=FALSE}

# Dataset containing the number of PCs we decided to keep (10)
final_dataset <- tags_pca$x[,c(1:10)]
dim(final_dataset) # Print dimensions

# Adding movieId column back to the dataset
features_pca <- data.frame(tb$movieId, final_dataset) # Adding movieId column

```

# Part 2: SVM and Neural Network fit

## Data Preprocessing

```{r packages_loading, include=FALSE, message = FALSE, warning=FALSE}

# Loading libraries
library(dplyr) # Data wrangling
library(ggplot2) # Visualizations
library(rsample) # Splitting Dataset
library(caret) # Efficient algorithms training
library(kernlab) # SVM
library(e1071) # SVM
library(recipes) # Compiling neural nets
library(keras) # Training & Building neural nets
library(tensorflow) # Training & Building neural nets
library(tfruns) # Training & Building neural nets
library(data.table) # For fast reading
library(stringr) # For string manipulation
```

Before fitting the ML algos we intend to (Support Vector Machine & Artificial
Neural Network), we need to **preprocess** our data to get the final dataset we
are going to use in our analysis. We want to use the **Principal Components** we
kept from Part 1 as the first **10 variables**. In addition, we want to create
**20 dummy variables**: those variables are gonna be binary variables and their
value is gonna depend on whether the movie belongs to a certain **genre** or not.
Finally, we are gonna add the **average rating** for each movie (the 31st
variable in our dataset), which is our **output variable** (the one we are gonna
try to predict).

Hence, we are going to import the dataset containing the average rating for each
movie (*ratings*) and the dataset containing the name and genre of each movie
(*movies*). Afterwards, we are going to **join** the latter two datasets with the
one containing the Principal Components.
 
```{r data_preprocessing, warning = FALSE, message = FALSE}

# Loading ratings dataset
ratings <- fread("C:/Users/optif/OneDrive/Desktop/academics/applied_statistics/Francesco_repository_1/data/ratings_actual.csv")

# Loading movies dataset
movies <- fread("C:/Users/optif/OneDrive/Desktop/academics/applied_statistics/Francesco_repository_1/data/movies_actual.csv")

# Joining ratings and movies data
movies_ratings <- left_join(ratings, movies, by = c("movieId"="movieId"))

# Joining movies_ratings and features_pca dataset
final_dataset <- left_join(features_pca, movies_ratings,
                           by = c("tb.movieId"="movieId"))

# Creating vector with all genres in dataset
genres_vec <- unique(unlist(strsplit(movies$genres, split = "\\|")))

# Adding dummy variables to final_dataset for each genre
for(i in genres_vec){
  final_dataset <- data.frame(final_dataset,
                              as.numeric(str_detect(final_dataset$genres, i)))
}

# Fixing column names in the final_dataset
colnames(final_dataset)[15:34] <- genres_vec

# Dropping columns we do not need
final_dataset <- final_dataset %>%
  select(-genres, -title, -tb.movieId)
```

# First Task: Support Vector Machine

```{r Dataset preparation, message = FALSE, warning=FALSE}

# Converting the column with average rating into a binary column
final_dataset_svm <- final_dataset %>%
  mutate(avg_rating = ifelse(avg_rating>=3.75, 1, 0))

# Split the data into training and testing set
set.seed(1234)
train_test <- initial_split(final_dataset_svm, prop = 0.8)
data_svm_train <- training(train_test)
data_svm_test <- testing(train_test)
```

Now we can try different types of SVM: Linear, Radial and Poly.

```{r SVM Radial, message = FALSE, warning=FALSE}

# Creating grid containing hyper parameters to tune
grid <- expand.grid(sigma = c(0.016, 0.017, 0.018, 0.019, 0.020),
                    C = c(1,2,3,4,5,6,7,8,9,10))

# Training Radial SVM
SVM_Radial <- caret::train(
  as.factor(avg_rating) ~ .,
  data = data_svm_train,
  method = "svmRadial", # Radial kernel      
  preProcess = c("center", "scale"),  # center & scale the data
  trControl = trainControl(method = "cv", number = 10), #cross-validation (10-fold) 
  tuneGrid = grid # grid created above
)

# Printing details about the Radial SVM tuned
print(SVM_Radial)

# Display training accuracy metrics
confusionMatrix(SVM_Radial)

# Model validation on the test set
Radial_test_validation <- predict(SVM_Radial, data_svm_test)
confusionMatrix(data = Radial_test_validation, as.factor(data_svm_test$avg_rating))

```

In the output of the SVM_Radial model, we can see that the highest accuracy is
achieved by setting the **Cost** hyper parameter at **5** and the **Sigma** hyper
parameter at **0.016**. The *out-of-sample* accuracy obtained with the SVM Radial
model and the stated hyper parameters is **82.02%**. We can now try to see if
Linear and Poly SVM can perform better than Radial SVM.

```{r SVM Poly, message = FALSE, warning=FALSE}

# Training Poly SVM
SVM_Poly <- caret::train(
  as.factor(avg_rating) ~ .,
  data = data_svm_train,
  method = "svmPoly",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("center","scale"),
  tuneLength = 4) # Parameter set to tune the 4 default values for the 3
                  # hyper parameters: degree, scale and C

# Print SVM_Poly details
print(SVM_Poly)

# Display training accuracy metrics
confusionMatrix(SVM_Poly)

# Model validation on the test set
Poly_test_validation <- predict(SVM_Poly, data_svm_test)
confusionMatrix(data = Poly_test_validation, as.factor(data_svm_test$avg_rating))
```

The **Poly SVM** performs slightly better than the **Radial SVM** out of sample,
since the out-of-sample accuracy is **82.19%**. The hyper parameters selected
based on the 10-fold cross validation are the following: **degree** of **2**,
**scale** of **0.1** and **Cost** of **0.5**. Now we can verify if the accuracy
improves by using a **Linear SVM**. In this case, though, we cannot use the
train function from the caret package since it is too computationally expensive
to train a Linear SVM with caret (we measured the time it takes). Hence we are
going to use the svm function from the e1071 package.

```{r SVM Linear Accuracy, message = FALSE, warning=FALSE}

# We are going to tune the only hyper parameter we have in the Linear SVM
# semi-automatically: by building a for loop testing for the values we want
# We first create a vector where to add the values of the Accuracy obtained
# at each step
Accuracy <- c()

# Here we build the for-loop
for(i in 1:10){
  SMV_Linear <- svm(as.factor(avg_rating) ~ .,
                    data = data_svm_train,
                    kernel = "linear",
                    cost = i,
                    scale = TRUE) # We choose to standardize the data

  # Test performance on out-of-sample data
  Linear_test_validation <- predict(SMV_Linear, data_svm_test)
  Lin_matrix <- confusionMatrix(data = Linear_test_validation,
                                as.factor(data_svm_test$avg_rating))
  
  # Add the Accuracy obtained to the Accuracy list
  Accuracy <- c(Accuracy, Lin_matrix$overall[1])
  
}

# Printing the values of Accuracy together with Cost values
Accuracy_dataset <- data.frame(seq(1,10,1), Accuracy)
Accuracy_dataset

```

As we can see by the dataset reporting the accuracy of the Linear SVM for the
different values of the **cost**, the best accuracy is achieved with a Cost higher
or equal to **2**: the model performs better for a **higher penalization** of the
*margin violations*. It is possible to see that the accuracy is the same for any
value of the cost that is at least equal to 2: **80.48%**. It is important to
highlight that the accuracy measured is **out-of-sample**. Based on the data, the
Linear SVM is the worst performing among all the SVM models built.

As we could see in all the three models built, though, *over fitting* is not an
issue: besides measuring accuracy in-sample, we also measured accuracy
out-of-sample and this actually turned out to be higher than the in-sample one.
In addition, the hyper parameters were selected using the 10-fold cross
validation procedure. Hence, the selection was performed taking into account the
**out-of-sample performance** reported at each iteration.

Now, we will compare the results obtained with another classifier algorithm, to
test the **relative effectiveness** of our SVM. The algorithm we are going to
use to compare our results is the **logistic regression**.

```{r Logistic Regression, message = FALSE, warning=FALSE}

# Training the Logistic model
logistic <- glm(avg_rating ~ .,
                family="binomial",
                data_svm_train)

# Printing summary of logistic
summary(logistic)

# We get the probability of being an excellent movie
prob_excellent<-predict(logistic, data_svm_test, type="response")

# Create the binary vector based on threshold 0.5
one_or_zero<-ifelse(prob_excellent>0.5,"1","0")

# Creating a vector of predictions
p_class<-factor(one_or_zero,levels=levels(as.factor(data_svm_test$avg_rating)))

# Computing and printing Confusion Matrix
con2<-confusionMatrix(p_class, as.factor(data_svm_test$avg_rating), positive="1")

# Printing Accuracy measure
con2$overall[1]
```
As we can see by the **accuracy** calculated **out-of-sample** (**80.65%**), the
logistic regression model performs *slightly worse* than the **Radial** and
**Poly SVM** but *slightly better* than the **Linear SVM**.

At the end of the exploration of all the models, we present our best performing
model: the Poly SVM, with a **degree** parameter of **2**, a **scale** parameter
of **0.1** and a **cost** parameter of **0.5**.

As the last step of the first task, we are going to re-tune our best SVM model
on the initial dataset changing the **threshold** for a rating to be considered
an excellent rating (from **3.75** to **4**), in order to see how the performance
changes.

```{r Poly SVM different threshold, message = FALSE, warning=FALSE}

# Getting data with threshold at 4
final_dataset_svm2 <- final_dataset %>%
  mutate(avg_rating = ifelse(avg_rating>=4, 1, 0))

# Split the data into training and testing set
set.seed(3456)
train_test2 <- initial_split(final_dataset_svm2, prop = 0.8)
data_svm_train2 <- training(train_test2)
data_svm_test2 <- testing(train_test2)

# Training Poly SVM
SVM_Poly2 <- caret::train(
  as.factor(avg_rating) ~ .,
  data = data_svm_train2,
  method = "svmPoly",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("center","scale"),
  tuneLength = 4) # Parameter set to tune the 4 default values for the 3
                  # hyper parameters: degree, scale and C

# Print SVM_Poly details
print(SVM_Poly2)

# Display training accuracy metrics
confusionMatrix(SVM_Poly2)

# Model validation on the test set
Poly_test_validation2 <- predict(SVM_Poly2, data_svm_test2)
confusionMatrix(data = Poly_test_validation2, as.factor(data_svm_test2$avg_rating))
```

After tuning the **Poly SVM** on the dataset again with a **different label**
(the new threshold for excellent rated movie set at 4), we can see that the
out-of-sample accuracy of the model is **higher** than the one obtained in the
previous case (threshold for excellent movie at 3.75): it is **92.5%** compared
to the **82.2%** obtained previously. In addition, the optimal hyper parameters
are different in this case: the **degree** is now **3**, the **scale** is **0.01**
and the **Cost** is **1**. In this model, a **higher penalization** for data points
over the boundary and a **higher grade** lead to a **higher accuracy**.

We could explain this result by thinking to the fact that a higher threshold
changes the distribution of "excellent" and "non-excellent" movies in the
D-dimensional space (D being 30 in this case), while all the predictive
variables stay the same. Most likely, this change in the "distribution" led to
less "overlap" between the two types of observations (excellent and
non-excellent), thus making it easier for the SVM to *linearly separate* the Data
in the infinite-dimensional space. This hypothesis is actually supported by a
higher **Cost** parameter compared to the one obtained previously: the model
allows for less *"violation"* of the margin, thus increasing
the accuracy of the model.

# Second Task: Artificial Neural Network

```{r Data Preprocessing Ann, message = FALSE, warning=FALSE}

# We first split the data
set.seed(2222)
train_test_split_ann <- initial_split(final_dataset, prop = 0.8)
train_tbl_ann <- training(train_test_split_ann)
test_tbl_ann  <- testing(train_test_split_ann)
cat("Dimensions of the training set is: ", dim(train_tbl_ann), "\n")

# Create the recipe object
recipe_obj <- recipe(avg_rating ~ ., data = train_tbl_ann) %>%
  step_center(all_numeric_predictors(), -all_outcomes()) %>%
  step_scale(all_numeric_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl_ann)

# Creating X and Y sets
x_train_ann <- bake(recipe_obj, new_data = train_tbl_ann) %>% select(-avg_rating)
x_test_ann  <- bake(recipe_obj, new_data = test_tbl_ann) %>% select(-avg_rating)
glimpse(x_train_ann)

y_train_vector <- train_tbl_ann$avg_rating
y_test_vector <- test_tbl_ann$avg_rating
```

Initially, we split the data into *training* (80%) and *testing* (20%) set and,
after this step, we pre-processed the data by **standardizing** the *numerical*
*predictive* variables. After adding the pre-processed data to the recipe object,
we extract the predictors and outcome variables for both train and test set. Now,
we are ready to fit the **neural network**.

```{r Fitting the Ann, message = FALSE, warning=FALSE}

# Creating the model_keras object
model_keras <- keras_model_sequential()

# Compiling the model_keras object with the architecture defined
model_keras %>% 
  
  # First hidden layer
  layer_dense(
    units              = 32, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_ann)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.3) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 64, 
    kernel_initializer = "uniform", 
    activation         = "tanh") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.3) %>%
  
  # Third hidden layer
  layer_dense(
    units              = 32,
    kernel_initializer = "uniform", 
    activation         = "tanh") %>%
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.3) %>%
  
  # Output layer
  layer_dense(
    units              = 1,
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Compile NN
  compile(
    optimizer = optimizer_rmsprop(),
    loss      = 'mean_squared_error',
    metrics   = c('mean_absolute_error')
  )

#display model architecture
model_keras

# Tuning the model and keeping the accuracy metrics
history_ann <- fit(
  object = model_keras,
  x = as.matrix(x_train_ann),
  y = y_train_vector,
  batch_size = 50,
  epochs = 50,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
)

# Printing and plotting history
print(history_ann)

plot(history_ann)
```

After tuning the model, we can now use it to predict **out-of-sample** data and
test its accuracy.

```{r Predicting with ANN, message = FALSE, warning=FALSE}

# Predicting using out-of-sample data
pred_test <- model_keras %>%
  predict(as.matrix(x_test_ann), batch_size = 50)

# Computing the RMSE for out-of-sample predictions
RMSE(pred_test, y_test_vector)
```

After tuning our Artificial Neural Network, we came up with our final
architecture and set of hyper parameters. The final Neural Networks has **3**
hidden layers and *one* output layer. The *first* and *third* layer of neurons
contain **32** neurons, whereas the second layer contains **64** neurons. The
activation function chosen for both the first and the last neurons is the **relu**
function: especially in the last layer, we want to keep an activation function
with a *non-vanishing gradient*: an activation function that does not entail
loss of information about the *input signal* due to its gradient-vanishing shape.
In addition, we decided to set a dropout percentage of **30%** in order to reduce
overfitting as much as we could (in fact, 30% is pretty high as a dropout
percentage). **Three** dropout layers have been placed after each hidden layer of
the neural network. In the end, we decided to set the optimizer as the **Moving**
**average of the square of gradients** (set by the function *optimizer_rmsprop()*),
a loss function as the mean squared error and a measure of accuracy as the mean
absolute error: the **mean squared error** is one of the functions that performs
most accurately with regression models.

In conclusion, we trained the model setting a *batch size* of **50** (number of
observations coming through the network at each epoch) and **50** *epochs* (number of
iterations of the batch through the model). Furthermore, we set the percentage of
observations left for *validation* at **30%**, in order to avoid overfitting as
much as possible.

The final **out-of-sample RMSE** obtained predicting ratings using the ANN is
approximately **0.29**.

```{r Comparing with LM, message = FALSE, warning=FALSE}

# Tuning Linear Regression Model
ln_regression <- lm(avg_rating ~ ., data=data_svm_train2)

# Calculating out-of-sample predictions for the Linear Model
lm_prediction <- predict(ln_regression, data_svm_test2)

# Calculating out-of-sample RMSE for Linear Regression
RMSE(lm_prediction, data_svm_test2$avg_rating)

```

Comparing the ANN with the performance of another regression model (linear
regression model), we see that the other model actually performs better than
the ANN: the **out-of-sample RMSE** of the linear regression is **0.2384**. This is 
most likely due to the fact that the relationship between the predictors and the
output variable is *approximately linear* and it is better modeled and predicted
by using a simple linear regression rather than a complex Neural Network: the
Neural Network is usually exploited for its potential to introduce *non-linearity*
in the model. In this case, though, linearity might be the solution.
